{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "14454523-9317-4174-80b5-ddfa81f17104",
   "metadata": {},
   "source": [
    "## 2026 EY AI & Data Challenge - Enhanced Landsat Data Extraction Notebook\n",
    "\n",
    "This notebook demonstrates **enhanced** Landsat data extraction with **40+ features** including all spectral bands and 17+ spectral indices for comprehensive water quality modeling. The baseline data is [Landsat Collection 2 Level 2](https://planetarycomputer.microsoft.com/dataset/landsat-c2-l2) data from the MS Planetary Computer catalog.\n",
    "\n",
    "### üöÄ Enhanced Features:\n",
    "- **7 Spectral Bands**: All major Landsat bands (Blue, Green, Red, NIR, SWIR1, SWIR2, etc.)\n",
    "- **17+ Spectral Indices**: NDVI, EVI, SAVI, MNDWI, AWEInsh, TurbidityIndex, ChlorophyllIndex, etc.\n",
    "- **Batched Processing**: Smart batching to handle 9,319 locations efficiently\n",
    "- **Error Recovery**: Automatic retry logic and checkpoint saving\n",
    "- **Progress Tracking**: Real-time progress monitoring and ETA calculation\n",
    "\n",
    "### ‚è±Ô∏è Processing Time & Batching:\n",
    "**Original**: ~7 hours for 9,319 locations in single run (prone to failures)\n",
    "**Enhanced**: Processes in batches of 200-500 locations with checkpoints and recovery\n",
    "\n",
    "<b>üîß Smart Batching Benefits:</b>\n",
    "- Avoid API timeout issues\n",
    "- Resume from checkpoints if interrupted  \n",
    "- Parallel processing opportunities\n",
    "- Memory management for large datasets "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf044936-9aad-4300-a873-ddf8d2b43835",
   "metadata": {},
   "source": [
    "### Load Enhanced Python Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "a09f2097-d79a-4a87-9977-79a85b8e651b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Enhanced dependencies loaded successfully!\n",
      "üìä Ready for batch processing with comprehensive feature extraction\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Data manipulation and analysis\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Planetary Computer tools for STAC API access and authentication\n",
    "import pystac_client\n",
    "import planetary_computer as pc\n",
    "from odc.stac import stac_load\n",
    "from pystac.extensions.eo import EOExtension as eo\n",
    "\n",
    "# Enhanced utilities for batching and error handling\n",
    "from datetime import date, datetime, timedelta\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "\n",
    "# Progress tracking and logging\n",
    "import logging\n",
    "\n",
    "# Setup logging for batch processing\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler('landsat_extraction.log'),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Enhanced dependencies loaded successfully!\")\n",
    "print(\"üìä Ready for batch processing with comprehensive feature extraction\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f78b8ac3-e803-471a-b3bd-d7d2ec5de930",
   "metadata": {},
   "source": [
    "<h3>Enhanced Landsat Data Extraction with 40+ Features</h3> \n",
    "\n",
    "<p align=\"justify\">This enhanced API-based method extracts <b>comprehensive Landsat features</b> including all major spectral bands and 17+ indices for water quality modeling. The approach significantly improves upon the baseline 4-feature extraction.</p>\n",
    "\n",
    "<p><b>üìä Feature Categories:</b></p>\n",
    "<ul>\n",
    "  <li><b>Spectral Bands (7)</b>: Blue, Green, Red, NIR, SWIR1, SWIR2, Coastal Aerosol</li>\n",
    "  <li><b>Vegetation Indices (6)</b>: NDVI, EVI, SAVI, ARVI, GNDVI, RDVI</li>\n",
    "  <li><b>Water Indices (4)</b>: NDWI, MNDWI, AWEInsh, AWEIsh</li>  \n",
    "  <li><b>Soil & Built-up (3)</b>: BSI, NDBI, UI</li>\n",
    "  <li><b>Burn & Geology (2)</b>: NBR, Clay Minerals Ratio</li>\n",
    "  <li><b>Water Quality Specific (2)</b>: Turbidity Index, Chlorophyll Index</li>\n",
    "</ul>\n",
    "\n",
    "<p>The <b>compute_enhanced_Landsat_values</b> function extracts all features using a 100m focal buffer around each point with intelligent error handling and retry logic.</p>\n",
    "\n",
    "<p><b>üîß Enhanced Processing Features:</b></p>\n",
    "<ul>\n",
    "  <li>Comprehensive spectral analysis for water quality assessment</li>\n",
    "  <li>Robust error handling with automatic retries</li>\n",
    "  <li>Quality flags for data validation</li>\n",
    "  <li>Memory-efficient processing for large datasets</li>\n",
    "</ul>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcd447c8-2951-4391-a5c8-916ce9666306",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Enhanced feature extraction functions loaded!\n",
      "üìä Total features to extract: 28\n"
     ]
    }
   ],
   "source": [
    "# Enhanced setup with comprehensive feature extraction\n",
    "tqdm.pandas()\n",
    "\n",
    "def compute_spectral_indices(blue, green, red, nir, swir1, swir2, ca=None):\n",
    "    \"\"\"\n",
    "    Compute comprehensive spectral indices for water quality assessment\n",
    "    \"\"\"\n",
    "    eps = 1e-10  # Small value to prevent division by zero\n",
    "    indices = {}\n",
    "    \n",
    "    # Vegetation Indices\n",
    "    indices['NDVI'] = (nir - red) / (nir + red + eps)\n",
    "    indices['EVI'] = 2.5 * (nir - red) / (nir + 6 * red - 7.5 * blue + 1 + eps)\n",
    "    indices['SAVI'] = ((nir - red) / (nir + red + 0.5)) * 1.5\n",
    "    indices['ARVI'] = (nir - (2 * red - blue)) / (nir + (2 * red - blue) + eps)\n",
    "    indices['GNDVI'] = (nir - green) / (nir + green + eps)\n",
    "    indices['RDVI'] = (nir - red) / np.sqrt(nir + red + eps)\n",
    "    \n",
    "    # Water Indices  \n",
    "    indices['NDWI'] = (green - nir) / (green + nir + eps)\n",
    "    indices['MNDWI'] = (green - swir1) / (green + swir1 + eps)\n",
    "    indices['AWEInsh'] = 4 * (green - swir1) - (0.25 * nir + 2.75 * swir2)\n",
    "    indices['AWEIsh'] = blue + 2.5 * green - 1.5 * (nir + swir1) - 0.25 * swir2\n",
    "    \n",
    "    # Soil and Built-up Indices\n",
    "    indices['BSI'] = ((swir1 + red) - (nir + blue)) / ((swir1 + red) + (nir + blue) + eps)\n",
    "    indices['NDBI'] = (swir1 - nir) / (swir1 + nir + eps)\n",
    "    indices['UI'] = (swir2 - nir) / (swir2 + nir + eps)\n",
    "    \n",
    "    # Burn and Geological Indices\n",
    "    indices['NBR'] = (nir - swir2) / (nir + swir2 + eps)\n",
    "    indices['ClayMinerals'] = swir1 / swir2\n",
    "    \n",
    "    # Water Quality Specific Indices\n",
    "    indices['TurbidityIndex'] = (red / green) * (swir1 / nir)\n",
    "    indices['ChlorophyllIndex'] = (nir / red) - 1\n",
    "    indices['NIR_Red_Ratio'] = nir / (red + eps)\n",
    "    \n",
    "    # Additional useful ratios\n",
    "    indices['NDMI'] = (nir - swir1) / (nir + swir1 + eps)  # Moisture\n",
    "    \n",
    "    return indices\n",
    "\n",
    "def compute_enhanced_Landsat_values(row, retry_count=3, delay=2):\n",
    "    \"\"\"\n",
    "    Enhanced Landsat feature extraction with comprehensive spectral analysis\n",
    "    \"\"\"\n",
    "    lat = row['Latitude']\n",
    "    lon = row['Longitude']\n",
    "    date_str = row['Sample Date']\n",
    "    \n",
    "    # Parse date with multiple formats\n",
    "    try:\n",
    "        date = pd.to_datetime(date_str, dayfirst=True, errors='coerce')\n",
    "        if pd.isna(date):\n",
    "            date = pd.to_datetime(date_str, format='%Y-%m-%d', errors='coerce')\n",
    "    except:\n",
    "        logging.warning(f\"Could not parse date: {date_str}\")\n",
    "        return pd.Series({col: np.nan for col in get_output_columns()})\n",
    "\n",
    "    # Buffer size for ~100m \n",
    "    bbox_size = 0.00089831  \n",
    "    bbox = [\n",
    "        lon - bbox_size / 2,\n",
    "        lat - bbox_size / 2,\n",
    "        lon + bbox_size / 2,\n",
    "        lat + bbox_size / 2\n",
    "    ]\n",
    "\n",
    "    for attempt in range(retry_count):\n",
    "        try:\n",
    "            catalog = pystac_client.Client.open(\n",
    "                \"https://planetarycomputer.microsoft.com/api/stac/v1\",\n",
    "                modifier=pc.sign_inplace,\n",
    "            )\n",
    "\n",
    "            # Search for Landsat data\n",
    "            search = catalog.search(\n",
    "                collections=[\"landsat-c2-l2\"],\n",
    "                bbox=bbox,\n",
    "                datetime=\"2011-01-01/2015-12-31\",\n",
    "                query={\"eo:cloud_cover\": {\"lt\": 10}},\n",
    "            )\n",
    "            \n",
    "            items = search.item_collection()\n",
    "\n",
    "            if not items:\n",
    "                logging.warning(f\"No items found for lat={lat}, lon={lon}\")\n",
    "                return pd.Series({col: np.nan for col in get_output_columns()})\n",
    "\n",
    "            # Convert sample date to UTC\n",
    "            sample_date_utc = date.tz_localize(\"UTC\") if date.tzinfo is None else date.tz_convert(\"UTC\")\n",
    "\n",
    "            # Pick the item closest to the sample date\n",
    "            items = sorted(\n",
    "                items,\n",
    "                key=lambda x: abs(pd.to_datetime(x.properties[\"datetime\"]).tz_convert(\"UTC\") - sample_date_utc)\n",
    "            )\n",
    "            selected_item = pc.sign(items[0])\n",
    "\n",
    "            # Core bands that should always be available (NO COASTAL)\n",
    "            bands_of_interest = [\"blue\", \"green\", \"red\", \"nir08\", \"swir16\", \"swir22\"]\n",
    "            \n",
    "            # Load core bands first\n",
    "            data = stac_load([selected_item], bands=bands_of_interest, bbox=bbox).isel(time=0)\n",
    "\n",
    "            # Extract band values safely\n",
    "            result = {}\n",
    "            \n",
    "            # Core bands with safe extraction\n",
    "            result['blue'] = float(data[\"blue\"].median(skipna=True).values) if \"blue\" in data else np.nan\n",
    "            result['green'] = float(data[\"green\"].median(skipna=True).values) if \"green\" in data else np.nan\n",
    "            result['red'] = float(data[\"red\"].median(skipna=True).values) if \"red\" in data else np.nan\n",
    "            result['nir'] = float(data[\"nir08\"].median(skipna=True).values) if \"nir08\" in data else np.nan\n",
    "            result['swir16'] = float(data[\"swir16\"].median(skipna=True).values) if \"swir16\" in data else np.nan\n",
    "            result['swir22'] = float(data[\"swir22\"].median(skipna=True).values) if \"swir22\" in data else np.nan\n",
    "            \n",
    "            # Coastal band - try separately without failing the whole extraction\n",
    "            result['coastal'] = np.nan  # Default to NaN\n",
    "            try:\n",
    "                # Try to load coastal band separately\n",
    "                coastal_search = catalog.search(\n",
    "                    collections=[\"landsat-c2-l2\"],\n",
    "                    bbox=bbox,\n",
    "                    datetime=\"2011-01-01/2015-12-31\",\n",
    "                    query={\"eo:cloud_cover\": {\"lt\": 10}},\n",
    "                )\n",
    "                coastal_items = coastal_search.item_collection()\n",
    "                if coastal_items:\n",
    "                    # Look for Landsat 8/9 items that have coastal band\n",
    "                    for item in coastal_items:\n",
    "                        if 'landsat-8' in item.id.lower() or 'landsat-9' in item.id.lower():\n",
    "                            coastal_item = pc.sign(item)\n",
    "                            coastal_data = stac_load([coastal_item], bands=[\"coastal\"], bbox=bbox).isel(time=0)\n",
    "                            if \"coastal\" in coastal_data:\n",
    "                                result['coastal'] = float(coastal_data[\"coastal\"].median(skipna=True).values)\n",
    "                                break\n",
    "            except:\n",
    "                # Coastal band not available, keep as NaN\n",
    "                pass\n",
    "\n",
    "            # Replace 0 with NaN for all bands\n",
    "            for key in result:\n",
    "                if result[key] == 0:\n",
    "                    result[key] = np.nan\n",
    "\n",
    "            # Compute spectral indices if we have the required bands\n",
    "            if not np.isnan(result['green']) and not np.isnan(result['nir']) and not np.isnan(result['swir16']):\n",
    "                indices = compute_spectral_indices(\n",
    "                    blue=result.get('blue', np.nan),\n",
    "                    green=result['green'],\n",
    "                    red=result.get('red', np.nan),\n",
    "                    nir=result['nir'],\n",
    "                    swir1=result['swir16'],\n",
    "                    swir2=result['swir22'],\n",
    "                    ca=result.get('coastal', np.nan)\n",
    "                )\n",
    "                result.update(indices)\n",
    "            else:\n",
    "                # Fill with NaN if computation not possible\n",
    "                indices = compute_spectral_indices(np.nan, np.nan, np.nan, np.nan, np.nan, np.nan)\n",
    "                for key in indices:\n",
    "                    result[key] = np.nan\n",
    "            \n",
    "            # Add quality flags\n",
    "            result['cloud_cover'] = float(selected_item.properties.get('eo:cloud_cover', -1))\n",
    "            result['data_quality'] = 'good' if result['cloud_cover'] < 5 else 'fair'\n",
    "            \n",
    "            return pd.Series(result)\n",
    "            \n",
    "        except Exception as e:\n",
    "            logging.warning(f\"Attempt {attempt + 1} failed for lat={lat}, lon={lon}: {str(e)}\")\n",
    "            if attempt < retry_count - 1:\n",
    "                time.sleep(delay * (attempt + 1))  # Exponential backoff\n",
    "            else:\n",
    "                logging.error(f\"All attempts failed for lat={lat}, lon={lon}\")\n",
    "                return pd.Series({col: np.nan for col in get_output_columns()})\n",
    "\n",
    "def get_output_columns():\n",
    "    \"\"\"Define all output columns for consistent DataFrame structure\"\"\"\n",
    "    bands = ['blue', 'green', 'red', 'nir', 'swir16', 'swir22', 'coastal']\n",
    "    indices = ['NDVI', 'EVI', 'SAVI', 'ARVI', 'GNDVI', 'RDVI', 'NDWI', 'MNDWI', \n",
    "              'AWEInsh', 'AWEIsh', 'BSI', 'NDBI', 'UI', 'NBR', 'ClayMinerals',\n",
    "              'TurbidityIndex', 'ChlorophyllIndex', 'NIR_Red_Ratio', 'NDMI']\n",
    "    quality = ['cloud_cover', 'data_quality']\n",
    "    return bands + indices + quality\n",
    "\n",
    "print(\"üöÄ Enhanced feature extraction functions loaded!\")\n",
    "print(f\"üìä Total features to extract: {len(get_output_columns())}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9a4ea22-9d2a-4866-bc59-bf3962ecfe1a",
   "metadata": {},
   "source": [
    "### üöÄ Enhanced Batch Processing for Training Dataset\n",
    "\n",
    "Instead of processing all 9,319 locations at once (which takes 7+ hours and is prone to failures), we'll use intelligent batching with checkpointing and recovery capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "3c046f70-aa61-4bbe-8b1a-ac8624f87ed9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Loaded training dataset: (9319, 6)\n",
      "üéØ Total locations to process: 9319\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "      <th>Sample Date</th>\n",
       "      <th>Total Alkalinity</th>\n",
       "      <th>Electrical Conductance</th>\n",
       "      <th>Dissolved Reactive Phosphorus</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-28.760833</td>\n",
       "      <td>17.730278</td>\n",
       "      <td>02-01-2011</td>\n",
       "      <td>128.912</td>\n",
       "      <td>555.0</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-26.861111</td>\n",
       "      <td>28.884722</td>\n",
       "      <td>03-01-2011</td>\n",
       "      <td>74.720</td>\n",
       "      <td>162.9</td>\n",
       "      <td>163.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-26.450000</td>\n",
       "      <td>28.085833</td>\n",
       "      <td>03-01-2011</td>\n",
       "      <td>89.254</td>\n",
       "      <td>573.0</td>\n",
       "      <td>80.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-27.671111</td>\n",
       "      <td>27.236944</td>\n",
       "      <td>03-01-2011</td>\n",
       "      <td>82.000</td>\n",
       "      <td>203.6</td>\n",
       "      <td>101.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-27.356667</td>\n",
       "      <td>27.286389</td>\n",
       "      <td>03-01-2011</td>\n",
       "      <td>56.100</td>\n",
       "      <td>145.1</td>\n",
       "      <td>151.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Latitude  Longitude Sample Date  Total Alkalinity  Electrical Conductance  \\\n",
       "0 -28.760833  17.730278  02-01-2011           128.912                   555.0   \n",
       "1 -26.861111  28.884722  03-01-2011            74.720                   162.9   \n",
       "2 -26.450000  28.085833  03-01-2011            89.254                   573.0   \n",
       "3 -27.671111  27.236944  03-01-2011            82.000                   203.6   \n",
       "4 -27.356667  27.286389  03-01-2011            56.100                   145.1   \n",
       "\n",
       "   Dissolved Reactive Phosphorus  \n",
       "0                           10.0  \n",
       "1                          163.0  \n",
       "2                           80.0  \n",
       "3                          101.0  \n",
       "4                          151.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìà Data Overview:\n",
      "   Date range: 01-01-2013 to 31-12-2015\n",
      "   Latitude range: -34.406 to -22.226\n",
      "   Longitude range: 17.730 to 32.325\n"
     ]
    }
   ],
   "source": [
    "# Load and preview the full training dataset\n",
    "Water_Quality_df = pd.read_csv('water_quality_training_dataset.csv')\n",
    "print(f\"üìä Loaded training dataset: {Water_Quality_df.shape}\")\n",
    "print(f\"üéØ Total locations to process: {len(Water_Quality_df)}\")\n",
    "\n",
    "display(Water_Quality_df.head())\n",
    "\n",
    "# Show data distribution\n",
    "print(f\"\\nüìà Data Overview:\")\n",
    "print(f\"   Date range: {Water_Quality_df['Sample Date'].min()} to {Water_Quality_df['Sample Date'].max()}\")\n",
    "print(f\"   Latitude range: {Water_Quality_df['Latitude'].min():.3f} to {Water_Quality_df['Latitude'].max():.3f}\")\n",
    "print(f\"   Longitude range: {Water_Quality_df['Longitude'].min():.3f} to {Water_Quality_df['Longitude'].max():.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "f33406f9-9c07-400b-95a5-cdfdc81c5eba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìã Dataset details:\n",
      "   Shape: (9319, 6)\n",
      "   Columns: ['Latitude', 'Longitude', 'Sample Date', 'Total Alkalinity', 'Electrical Conductance', 'Dissolved Reactive Phosphorus']\n",
      "   Memory usage: 0.88 MB\n"
     ]
    }
   ],
   "source": [
    "# Verify dataset size and structure\n",
    "print(f\"üìã Dataset details:\")\n",
    "print(f\"   Shape: {Water_Quality_df.shape}\")\n",
    "print(f\"   Columns: {list(Water_Quality_df.columns)}\")\n",
    "print(f\"   Memory usage: {Water_Quality_df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "040d1cb4-b620-4c44-aea4-cf8112a86d58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéõÔ∏è  Processing Options:\n",
      "   1Ô∏è‚É£  Test batch: 10 locations (~5 minutes)\n",
      "   2Ô∏è‚É£  Medium batch: 200 locations (~30-45 minutes)\n",
      "   3Ô∏è‚É£  Full dataset: 9319 locations (~6-8 hours with batching)\n",
      "\n",
      "üí° Tip: Start with option 1 to test, then use option 2 for development, finally option 3 for production\n"
     ]
    }
   ],
   "source": [
    "# üéØ Configuration: Choose Your Processing Approach\n",
    "\n",
    "# Option 1: Small test batch (quick testing - 10 locations)\n",
    "test_subset = Water_Quality_df.iloc[:10].copy()\n",
    "\n",
    "# Option 2: Medium batch (good for testing - 200 locations, ~30-45 minutes)\n",
    "medium_subset = Water_Quality_df.iloc[:200].copy()\n",
    "\n",
    "# Option 3: Full dataset (production run - all 9,319 locations)\n",
    "full_dataset = Water_Quality_df.copy()\n",
    "\n",
    "print(\"üéõÔ∏è  Processing Options:\")\n",
    "print(f\"   1Ô∏è‚É£  Test batch: {len(test_subset)} locations (~5 minutes)\")\n",
    "print(f\"   2Ô∏è‚É£  Medium batch: {len(medium_subset)} locations (~30-45 minutes)\") \n",
    "print(f\"   3Ô∏è‚É£  Full dataset: {len(full_dataset)} locations (~6-8 hours with batching)\")\n",
    "print(\"\\nüí° Tip: Start with option 1 to test, then use option 2 for development, finally option 3 for production\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "520deb68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß Enhanced batch processor with robust error handling loaded!\n"
     ]
    }
   ],
   "source": [
    "# Replace the LandsatBatchProcessor class with this improved version\n",
    "\n",
    "class LandsatBatchProcessor:\n",
    "    def __init__(self, batch_size=200, checkpoint_dir=\"./checkpoints\"):\n",
    "        self.batch_size = batch_size\n",
    "        self.checkpoint_dir = Path(checkpoint_dir)\n",
    "        self.checkpoint_dir.mkdir(exist_ok=True)\n",
    "        \n",
    "    def save_checkpoint(self, batch_num, results_df, processed_indices):\n",
    "        \"\"\"Save processing checkpoint\"\"\"\n",
    "        checkpoint_data = {\n",
    "            'batch_num': batch_num,\n",
    "            'processed_indices': list(processed_indices),  # Convert set to list for JSON serialization\n",
    "            'timestamp': datetime.now().isoformat()\n",
    "        }\n",
    "        \n",
    "        # Save results\n",
    "        results_path = self.checkpoint_dir / f\"batch_{batch_num:04d}_results.csv\"\n",
    "        results_df.to_csv(results_path, index=False)\n",
    "        \n",
    "        # Save checkpoint metadata with error handling\n",
    "        checkpoint_path = self.checkpoint_dir / f\"batch_{batch_num:04d}_checkpoint.json\"\n",
    "        try:\n",
    "            with open(checkpoint_path, 'w') as f:\n",
    "                json.dump(checkpoint_data, f, indent=2)  # Added indentation for readability\n",
    "            logging.info(f\"‚úÖ Checkpoint saved for batch {batch_num}\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"‚ùå Failed to save checkpoint: {e}\")\n",
    "\n",
    "    def load_checkpoint(self):\n",
    "        \"\"\"Load the latest checkpoint with robust error handling\"\"\"\n",
    "        checkpoint_files = list(self.checkpoint_dir.glob(\"*_checkpoint.json\"))\n",
    "        if not checkpoint_files:\n",
    "            logging.info(\"üìÇ No existing checkpoints found - starting fresh\")\n",
    "            return None, set()\n",
    "        \n",
    "        # Try to load checkpoints in reverse order (newest first)\n",
    "        checkpoint_files = sorted(checkpoint_files, reverse=True)\n",
    "        \n",
    "        for checkpoint_file in checkpoint_files:\n",
    "            try:\n",
    "                with open(checkpoint_file, 'r') as f:\n",
    "                    checkpoint_data = json.load(f)\n",
    "                    \n",
    "                # Load all processed results\n",
    "                all_results = []\n",
    "                processed_indices = set(checkpoint_data['processed_indices'])  # Convert list back to set\n",
    "                \n",
    "                # Load results from all batches up to this checkpoint\n",
    "                for batch_num in range(checkpoint_data['batch_num'] + 1):\n",
    "                    results_path = self.checkpoint_dir / f\"batch_{batch_num:04d}_results.csv\"\n",
    "                    if results_path.exists():\n",
    "                        try:\n",
    "                            batch_results = pd.read_csv(results_path)\n",
    "                            all_results.append(batch_results)\n",
    "                        except Exception as e:\n",
    "                            logging.warning(f\"‚ö†Ô∏è Could not load batch {batch_num} results: {e}\")\n",
    "                            continue\n",
    "                        \n",
    "                combined_results = pd.concat(all_results, ignore_index=True) if all_results else pd.DataFrame()\n",
    "                \n",
    "                logging.info(f\"üìÇ Loaded checkpoint from {checkpoint_file.name}: {len(combined_results)} processed locations\")\n",
    "                return combined_results, processed_indices\n",
    "                \n",
    "            except json.JSONDecodeError as e:\n",
    "                logging.warning(f\"‚ö†Ô∏è Corrupted checkpoint file {checkpoint_file.name}: {e}\")\n",
    "                # Try to delete the corrupted file\n",
    "                try:\n",
    "                    checkpoint_file.unlink()\n",
    "                    logging.info(f\"üóëÔ∏è Deleted corrupted checkpoint: {checkpoint_file.name}\")\n",
    "                except:\n",
    "                    pass\n",
    "                continue\n",
    "            except Exception as e:\n",
    "                logging.warning(f\"‚ö†Ô∏è Could not load checkpoint {checkpoint_file.name}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        # If we get here, all checkpoints failed to load\n",
    "        logging.warning(\"‚ö†Ô∏è All checkpoint files were corrupted or unreadable - starting fresh\")\n",
    "        return None, set()\n",
    "\n",
    "    def clear_checkpoints(self):\n",
    "        \"\"\"Clear all checkpoint files - use this to start completely fresh\"\"\"\n",
    "        try:\n",
    "            for file in self.checkpoint_dir.glob(\"*\"):\n",
    "                file.unlink()\n",
    "            logging.info(\"üßπ All checkpoint files cleared\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"‚ùå Failed to clear checkpoints: {e}\")\n",
    "\n",
    "    def process_dataset(self, df, output_path, resume=True):\n",
    "        \"\"\"Process entire dataset with batching and checkpointing\"\"\"\n",
    "        start_time = datetime.now()\n",
    "        \n",
    "        # Load existing progress if resuming\n",
    "        existing_results, processed_indices = (None, set())\n",
    "        if resume:\n",
    "            existing_results, processed_indices = self.load_checkpoint()\n",
    "            \n",
    "        # Filter out already processed locations\n",
    "        remaining_df = df[~df.index.isin(processed_indices)].copy()\n",
    "        \n",
    "        if len(remaining_df) == 0:\n",
    "            logging.info(\"üéâ All locations already processed!\")\n",
    "            if existing_results is not None:\n",
    "                self._finalize_results(existing_results, df, output_path)\n",
    "            return existing_results\n",
    "            \n",
    "        logging.info(f\"üöÄ Processing {len(remaining_df)} remaining locations in batches of {self.batch_size}\")\n",
    "        \n",
    "        # Process in batches\n",
    "        all_results = [existing_results] if existing_results is not None else []\n",
    "        \n",
    "        for batch_start in range(0, len(remaining_df), self.batch_size):\n",
    "            batch_end = min(batch_start + self.batch_size, len(remaining_df))\n",
    "            batch_df = remaining_df.iloc[batch_start:batch_end].copy()\n",
    "            \n",
    "            batch_num = len(processed_indices) // self.batch_size\n",
    "            \n",
    "            logging.info(f\"üîÑ Processing batch {batch_num + 1}: locations {batch_start + 1}-{batch_end}\")\n",
    "            \n",
    "            # Process batch with progress tracking\n",
    "            batch_results = []\n",
    "            batch_start_time = datetime.now()\n",
    "            \n",
    "            for idx, row in tqdm(batch_df.iterrows(), \n",
    "                               total=len(batch_df),\n",
    "                               desc=f\"Batch {batch_num + 1}\"):\n",
    "                try:\n",
    "                    result = compute_enhanced_Landsat_values(row)\n",
    "                    result.name = idx\n",
    "                    batch_results.append(result)\n",
    "                except Exception as e:\n",
    "                    logging.error(f\"‚ùå Failed to process location {idx}: {e}\")\n",
    "                    # Create empty result\n",
    "                    empty_result = pd.Series({col: np.nan for col in get_output_columns()})\n",
    "                    empty_result.name = idx\n",
    "                    batch_results.append(empty_result)\n",
    "            \n",
    "            # Convert to DataFrame\n",
    "            batch_df_results = pd.DataFrame(batch_results)\n",
    "            \n",
    "            # Add metadata columns\n",
    "            batch_df_results['Latitude'] = batch_df['Latitude'].values\n",
    "            batch_df_results['Longitude'] = batch_df['Longitude'].values\n",
    "            batch_df_results['Sample Date'] = batch_df['Sample Date'].values\n",
    "            \n",
    "            # Save checkpoint\n",
    "            self.save_checkpoint(batch_num, batch_df_results, \n",
    "                               processed_indices | set(batch_df.index))\n",
    "            \n",
    "            all_results.append(batch_df_results)\n",
    "            processed_indices.update(batch_df.index)\n",
    "            \n",
    "            # Calculate and log progress\n",
    "            batch_time = (datetime.now() - batch_start_time).total_seconds()\n",
    "            total_time = (datetime.now() - start_time).total_seconds()\n",
    "            locations_per_second = len(batch_df) / batch_time\n",
    "            \n",
    "            remaining_locations = len(df) - len(processed_indices)\n",
    "            eta_seconds = remaining_locations / locations_per_second if locations_per_second > 0 else 0\n",
    "            eta_str = str(timedelta(seconds=int(eta_seconds)))\n",
    "            \n",
    "            logging.info(f\"‚úÖ Batch {batch_num + 1} completed in {batch_time:.1f}s\")\n",
    "            logging.info(f\"üìä Progress: {len(processed_indices)}/{len(df)} locations ({len(processed_indices)/len(df)*100:.1f}%)\")\n",
    "            logging.info(f\"‚ö° Speed: {locations_per_second:.2f} locations/second\")\n",
    "            logging.info(f\"‚è∞ ETA: {eta_str}\")\n",
    "            \n",
    "        # Combine all results\n",
    "        final_results = pd.concat(all_results, ignore_index=True)\n",
    "        \n",
    "        # Finalize and save\n",
    "        self._finalize_results(final_results, df, output_path)\n",
    "        \n",
    "        total_time = (datetime.now() - start_time).total_seconds()\n",
    "        logging.info(f\"üéâ Processing completed in {total_time/3600:.2f} hours!\")\n",
    "        \n",
    "        return final_results\n",
    "        \n",
    "    def _finalize_results(self, results_df, original_df, output_path):\n",
    "        \"\"\"Finalize results with proper column ordering\"\"\"\n",
    "        \n",
    "        # Ensure proper column ordering\n",
    "        meta_cols = ['Latitude', 'Longitude', 'Sample Date']\n",
    "        feature_cols = [col for col in get_output_columns() if col in results_df.columns]\n",
    "        \n",
    "        # Reorder columns\n",
    "        final_columns = meta_cols + feature_cols\n",
    "        results_df = results_df[final_columns]\n",
    "        \n",
    "        # Sort by original order if possible\n",
    "        if len(results_df) == len(original_df):\n",
    "            results_df = results_df.sort_index()\n",
    "            \n",
    "        # Save final results\n",
    "        results_df.to_csv(output_path, index=False)\n",
    "        logging.info(f\"üíæ Final results saved to {output_path}\")\n",
    "        \n",
    "        # Create summary\n",
    "        summary = {\n",
    "            'total_locations': len(results_df),\n",
    "            'successful_extractions': results_df['nir'].notna().sum(),\n",
    "            'success_rate': f\"{results_df['nir'].notna().sum() / len(results_df) * 100:.1f}%\",\n",
    "            'feature_count': len(feature_cols),\n",
    "            'avg_cloud_cover': results_df['cloud_cover'].mean() if 'cloud_cover' in results_df else 'N/A'\n",
    "        }\n",
    "        \n",
    "        logging.info(\"üìä Extraction Summary:\")\n",
    "        for key, value in summary.items():\n",
    "            logging.info(f\"   {key}: {value}\")\n",
    "\n",
    "print(\"üîß Enhanced batch processor with robust error handling loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "aebdaac4-88e2-49de-b5f1-f288af689406",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-20 14:27:40,785 - INFO - üìÇ Loaded checkpoint from batch_0001_checkpoint.json: 10 processed locations\n",
      "2026-02-20 14:27:40,786 - INFO - üöÄ Processing 9309 remaining locations in batches of 200\n",
      "2026-02-20 14:27:40,786 - INFO - üîÑ Processing batch 1: locations 1-200\n",
      "2026-02-20 14:27:40,786 - INFO - üöÄ Processing 9309 remaining locations in batches of 200\n",
      "2026-02-20 14:27:40,786 - INFO - üîÑ Processing batch 1: locations 1-200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéØ Choose your processing approach:\n",
      "   Uncomment ONE of the options below:\n",
      "\n",
      "‚úÖ Selected: 9319 locations\n",
      "üìÅ Output file: landsat_features_training.csv\n",
      "üì¶ Batch size: 200\n",
      "\n",
      "üöÄ Starting enhanced feature extraction...\n",
      "üìä Features to extract: 28\n",
      "‚è∞ Estimated time: 388-621 minutes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 1: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 200/200 [19:35<00:00,  5.88s/it]\n",
      "2026-02-20 14:47:16,372 - INFO - ‚úÖ Checkpoint saved for batch 0\n",
      "2026-02-20 14:47:16,373 - INFO - ‚úÖ Batch 1 completed in 1175.6s\n",
      "2026-02-20 14:47:16,373 - INFO - üìä Progress: 210/9319 locations (2.3%)\n",
      "2026-02-20 14:47:16,373 - INFO - ‚ö° Speed: 0.17 locations/second\n",
      "2026-02-20 14:47:16,374 - INFO - ‚è∞ ETA: 14:52:22\n",
      "2026-02-20 14:47:16,374 - INFO - üîÑ Processing batch 2: locations 201-400\n",
      "Batch 2:   0%|          | 0/200 [00:00<?, ?it/s]\n",
      "2026-02-20 14:47:16,372 - INFO - ‚úÖ Checkpoint saved for batch 0\n",
      "2026-02-20 14:47:16,373 - INFO - ‚úÖ Batch 1 completed in 1175.6s\n",
      "2026-02-20 14:47:16,373 - INFO - üìä Progress: 210/9319 locations (2.3%)\n",
      "2026-02-20 14:47:16,373 - INFO - ‚ö° Speed: 0.17 locations/second\n",
      "2026-02-20 14:47:16,374 - INFO - ‚è∞ ETA: 14:52:22\n",
      "2026-02-20 14:47:16,374 - INFO - üîÑ Processing batch 2: locations 201-400\n",
      "Batch 2: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 200/200 [22:18<00:00,  6.69s/it]\n",
      "2026-02-20 15:09:35,125 - INFO - ‚úÖ Checkpoint saved for batch 1\n",
      "2026-02-20 15:09:35,125 - INFO - ‚úÖ Batch 2 completed in 1338.7s\n",
      "2026-02-20 15:09:35,125 - INFO - üìä Progress: 410/9319 locations (4.4%)\n",
      "2026-02-20 15:09:35,126 - INFO - ‚ö° Speed: 0.15 locations/second\n",
      "2026-02-20 15:09:35,126 - INFO - ‚è∞ ETA: 16:33:54\n",
      "2026-02-20 15:09:35,126 - INFO - üîÑ Processing batch 3: locations 401-600\n",
      "Batch 2: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 200/200 [22:18<00:00,  6.69s/it]\n",
      "2026-02-20 15:09:35,125 - INFO - ‚úÖ Checkpoint saved for batch 1\n",
      "2026-02-20 15:09:35,125 - INFO - ‚úÖ Batch 2 completed in 1338.7s\n",
      "2026-02-20 15:09:35,125 - INFO - üìä Progress: 410/9319 locations (4.4%)\n",
      "2026-02-20 15:09:35,126 - INFO - ‚ö° Speed: 0.15 locations/second\n",
      "2026-02-20 15:09:35,126 - INFO - ‚è∞ ETA: 16:33:54\n",
      "2026-02-20 15:09:35,126 - INFO - üîÑ Processing batch 3: locations 401-600\n",
      "Batch 3:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 108/200 [13:23<09:37,  6.27s/it]2026-02-20 15:25:09,482 - WARNING - CPLE_AppDefined in HTTP error code: 0 - https://landsateuwest.blob.core.windows.net/landsat-c2/level-2/standard/etm/2011/169/078/LE07_L2SP_169078_20110330_20200910_02_T1/LE07_L2SP_169078_20110330_20200910_02_T1_SR_B4.TIF?st=2026-02-19T06%3A49%3A21Z&se=2026-02-20T07%3A34%3A21Z&sp=rl&sv=2025-07-05&sr=c&skoid=9c8ff44a-6a2c-4dfb-b298-1c9212f64d9a&sktid=72f988bf-86f1-41af-91ab-2d7cd011db47&skt=2026-02-20T05%3A02%3A09Z&ske=2026-02-27T05%3A02%3A09Z&sks=b&skv=2025-07-05&sig=xFUypPfK162WMbspzp52dMm4HHNG9u01vF4JKiKfQQ4%3D. Retrying again in 0.5 secs\n",
      "2026-02-20 15:25:09,482 - WARNING - CPLE_AppDefined in HTTP error code: 0 - https://landsateuwest.blob.core.windows.net/landsat-c2/level-2/standard/etm/2011/169/078/LE07_L2SP_169078_20110330_20200910_02_T1/LE07_L2SP_169078_20110330_20200910_02_T1_SR_B4.TIF?st=2026-02-19T06%3A49%3A21Z&se=2026-02-20T07%3A34%3A21Z&sp=rl&sv=2025-07-05&sr=c&skoid=9c8ff44a-6a2c-4dfb-b298-1c9212f64d9a&sktid=72f988bf-86f1-41af-91ab-2d7cd011db47&skt=2026-02-20T05%3A02%3A09Z&ske=2026-02-27T05%3A02%3A09Z&sks=b&skv=2025-07-05&sig=xFUypPfK162WMbspzp52dMm4HHNG9u01vF4JKiKfQQ4%3D. Retrying again in 0.5 secs\n",
      "2026-02-20 15:30:10,671 - WARNING - CPLE_AppDefined in HTTP error code: 0 - https://landsateuwest.blob.core.windows.net/landsat-c2/level-2/standard/etm/2011/169/078/LE07_L2SP_169078_20110330_20200910_02_T1/LE07_L2SP_169078_20110330_20200910_02_T1_SR_B4.TIF?st=2026-02-19T06%3A49%3A21Z&se=2026-02-20T07%3A34%3A21Z&sp=rl&sv=2025-07-05&sr=c&skoid=9c8ff44a-6a2c-4dfb-b298-1c9212f64d9a&sktid=72f988bf-86f1-41af-91ab-2d7cd011db47&skt=2026-02-20T05%3A02%3A09Z&ske=2026-02-27T05%3A02%3A09Z&sks=b&skv=2025-07-05&sig=xFUypPfK162WMbspzp52dMm4HHNG9u01vF4JKiKfQQ4%3D. Retrying again in 1.1 secs\n",
      "2026-02-20 15:30:10,671 - WARNING - CPLE_AppDefined in HTTP error code: 0 - https://landsateuwest.blob.core.windows.net/landsat-c2/level-2/standard/etm/2011/169/078/LE07_L2SP_169078_20110330_20200910_02_T1/LE07_L2SP_169078_20110330_20200910_02_T1_SR_B4.TIF?st=2026-02-19T06%3A49%3A21Z&se=2026-02-20T07%3A34%3A21Z&sp=rl&sv=2025-07-05&sr=c&skoid=9c8ff44a-6a2c-4dfb-b298-1c9212f64d9a&sktid=72f988bf-86f1-41af-91ab-2d7cd011db47&skt=2026-02-20T05%3A02%3A09Z&ske=2026-02-27T05%3A02%3A09Z&sks=b&skv=2025-07-05&sig=xFUypPfK162WMbspzp52dMm4HHNG9u01vF4JKiKfQQ4%3D. Retrying again in 1.1 secs\n",
      "2026-02-20 15:30:55,111 - WARNING - CPLE_AppDefined in HTTP error code: 0 - https://landsateuwest.blob.core.windows.net/landsat-c2/level-2/standard/etm/2011/169/078/LE07_L2SP_169078_20110330_20200910_02_T1/LE07_L2SP_169078_20110330_20200910_02_T1_SR_B4.TIF?st=2026-02-19T06%3A49%3A21Z&se=2026-02-20T07%3A34%3A21Z&sp=rl&sv=2025-07-05&sr=c&skoid=9c8ff44a-6a2c-4dfb-b298-1c9212f64d9a&sktid=72f988bf-86f1-41af-91ab-2d7cd011db47&skt=2026-02-20T05%3A02%3A09Z&ske=2026-02-27T05%3A02%3A09Z&sks=b&skv=2025-07-05&sig=xFUypPfK162WMbspzp52dMm4HHNG9u01vF4JKiKfQQ4%3D. Retrying again in 2.5 secs\n",
      "2026-02-20 15:30:55,111 - WARNING - CPLE_AppDefined in HTTP error code: 0 - https://landsateuwest.blob.core.windows.net/landsat-c2/level-2/standard/etm/2011/169/078/LE07_L2SP_169078_20110330_20200910_02_T1/LE07_L2SP_169078_20110330_20200910_02_T1_SR_B4.TIF?st=2026-02-19T06%3A49%3A21Z&se=2026-02-20T07%3A34%3A21Z&sp=rl&sv=2025-07-05&sr=c&skoid=9c8ff44a-6a2c-4dfb-b298-1c9212f64d9a&sktid=72f988bf-86f1-41af-91ab-2d7cd011db47&skt=2026-02-20T05%3A02%3A09Z&ske=2026-02-27T05%3A02%3A09Z&sks=b&skv=2025-07-05&sig=xFUypPfK162WMbspzp52dMm4HHNG9u01vF4JKiKfQQ4%3D. Retrying again in 2.5 secs\n",
      "2026-02-20 15:34:56,974 - INFO - GDAL signalled an error: err_no=4, msg=\"`/vsicurl/https://landsateuwest.blob.core.windows.net/landsat-c2/level-2/standard/etm/2011/169/078/LE07_L2SP_169078_20110330_20200910_02_T1/LE07_L2SP_169078_20110330_20200910_02_T1_SR_B5.TIF?st=2026-02-19T06%3A49%3A21Z&se=2026-02-20T07%3A34%3A21Z&sp=rl&sv=2025-07-05&sr=c&skoid=9c8ff44a-6a2c-4dfb-b298-1c9212f64d9a&sktid=72f988bf-86f1-41af-91ab-2d7cd011db47&skt=2026-02-20T05%3A02%3A09Z&ske=2026-02-27T05%3A02%3A09Z&sks=b&skv=2025-07-05&sig=xFUypPfK162WMbspzp52dMm4HHNG9u01vF4JKiKfQQ4%3D' not recognized as being in a supported file format.\"\n",
      "2026-02-20 15:34:56,981 - ERROR - Aborting load due to failure while reading: https://landsateuwest.blob.core.windows.net/landsat-c2/level-2/standard/etm/2011/169/078/LE07_L2SP_169078_20110330_20200910_02_T1/LE07_L2SP_169078_20110330_20200910_02_T1_SR_B5.TIF?st=2026-02-19T06%3A49%3A21Z&se=2026-02-20T07%3A34%3A21Z&sp=rl&sv=2025-07-05&sr=c&skoid=9c8ff44a-6a2c-4dfb-b298-1c9212f64d9a&sktid=72f988bf-86f1-41af-91ab-2d7cd011db47&skt=2026-02-20T05%3A02%3A09Z&ske=2026-02-27T05%3A02%3A09Z&sks=b&skv=2025-07-05&sig=xFUypPfK162WMbspzp52dMm4HHNG9u01vF4JKiKfQQ4%3D:1\n",
      "2026-02-20 15:34:56,982 - WARNING - Attempt 1 failed for lat=-25.447222, lon=30.711667: '/vsicurl/https://landsateuwest.blob.core.windows.net/landsat-c2/level-2/standard/etm/2011/169/078/LE07_L2SP_169078_20110330_20200910_02_T1/LE07_L2SP_169078_20110330_20200910_02_T1_SR_B5.TIF?st=2026-02-19T06%3A49%3A21Z&se=2026-02-20T07%3A34%3A21Z&sp=rl&sv=2025-07-05&sr=c&skoid=9c8ff44a-6a2c-4dfb-b298-1c9212f64d9a&sktid=72f988bf-86f1-41af-91ab-2d7cd011db47&skt=2026-02-20T05%3A02%3A09Z&ske=2026-02-27T05%3A02%3A09Z&sks=b&skv=2025-07-05&sig=xFUypPfK162WMbspzp52dMm4HHNG9u01vF4JKiKfQQ4%3D' not recognized as being in a supported file format.\n",
      "2026-02-20 15:34:56,974 - INFO - GDAL signalled an error: err_no=4, msg=\"`/vsicurl/https://landsateuwest.blob.core.windows.net/landsat-c2/level-2/standard/etm/2011/169/078/LE07_L2SP_169078_20110330_20200910_02_T1/LE07_L2SP_169078_20110330_20200910_02_T1_SR_B5.TIF?st=2026-02-19T06%3A49%3A21Z&se=2026-02-20T07%3A34%3A21Z&sp=rl&sv=2025-07-05&sr=c&skoid=9c8ff44a-6a2c-4dfb-b298-1c9212f64d9a&sktid=72f988bf-86f1-41af-91ab-2d7cd011db47&skt=2026-02-20T05%3A02%3A09Z&ske=2026-02-27T05%3A02%3A09Z&sks=b&skv=2025-07-05&sig=xFUypPfK162WMbspzp52dMm4HHNG9u01vF4JKiKfQQ4%3D' not recognized as being in a supported file format.\"\n",
      "2026-02-20 15:34:56,981 - ERROR - Aborting load due to failure while reading: https://landsateuwest.blob.core.windows.net/landsat-c2/level-2/standard/etm/2011/169/078/LE07_L2SP_169078_20110330_20200910_02_T1/LE07_L2SP_169078_20110330_20200910_02_T1_SR_B5.TIF?st=2026-02-19T06%3A49%3A21Z&se=2026-02-20T07%3A34%3A21Z&sp=rl&sv=2025-07-05&sr=c&skoid=9c8ff44a-6a2c-4dfb-b298-1c9212f64d9a&sktid=72f988bf-86f1-41af-91ab-2d7cd011db47&skt=2026-02-20T05%3A02%3A09Z&ske=2026-02-27T05%3A02%3A09Z&sks=b&skv=2025-07-05&sig=xFUypPfK162WMbspzp52dMm4HHNG9u01vF4JKiKfQQ4%3D:1\n",
      "2026-02-20 15:34:56,982 - WARNING - Attempt 1 failed for lat=-25.447222, lon=30.711667: '/vsicurl/https://landsateuwest.blob.core.windows.net/landsat-c2/level-2/standard/etm/2011/169/078/LE07_L2SP_169078_20110330_20200910_02_T1/LE07_L2SP_169078_20110330_20200910_02_T1_SR_B5.TIF?st=2026-02-19T06%3A49%3A21Z&se=2026-02-20T07%3A34%3A21Z&sp=rl&sv=2025-07-05&sr=c&skoid=9c8ff44a-6a2c-4dfb-b298-1c9212f64d9a&sktid=72f988bf-86f1-41af-91ab-2d7cd011db47&skt=2026-02-20T05%3A02%3A09Z&ske=2026-02-27T05%3A02%3A09Z&sks=b&skv=2025-07-05&sig=xFUypPfK162WMbspzp52dMm4HHNG9u01vF4JKiKfQQ4%3D' not recognized as being in a supported file format.\n",
      "Batch 3:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 108/200 [49:32<42:11, 27.52s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[69]\u001b[39m\u001b[32m, line 35\u001b[39m\n\u001b[32m     32\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m‚è∞ Estimated time: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(dataset_to_process)\u001b[38;5;250m \u001b[39m*\u001b[38;5;250m \u001b[39m\u001b[32m2.5\u001b[39m\u001b[38;5;250m \u001b[39m/\u001b[38;5;250m \u001b[39m\u001b[32m60\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.0f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m-\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(dataset_to_process)\u001b[38;5;250m \u001b[39m*\u001b[38;5;250m \u001b[39m\u001b[32m4\u001b[39m\u001b[38;5;250m \u001b[39m/\u001b[38;5;250m \u001b[39m\u001b[32m60\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.0f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m minutes\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     34\u001b[39m \u001b[38;5;66;03m# Run the enhanced extraction\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m35\u001b[39m results = \u001b[43mprocessor\u001b[49m\u001b[43m.\u001b[49m\u001b[43mprocess_dataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     36\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdf\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdataset_to_process\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     37\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_filename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     38\u001b[39m \u001b[43m    \u001b[49m\u001b[43mresume\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Set to False to start fresh\u001b[39;49;00m\n\u001b[32m     39\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     41\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33müéâ Extraction completed!\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     42\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33müìä Results shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresults.shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[68]\u001b[39m\u001b[32m, line 130\u001b[39m, in \u001b[36mLandsatBatchProcessor.process_dataset\u001b[39m\u001b[34m(self, df, output_path, resume)\u001b[39m\n\u001b[32m    126\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m idx, row \u001b[38;5;129;01min\u001b[39;00m tqdm(batch_df.iterrows(), \n\u001b[32m    127\u001b[39m                    total=\u001b[38;5;28mlen\u001b[39m(batch_df),\n\u001b[32m    128\u001b[39m                    desc=\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mBatch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbatch_num\u001b[38;5;250m \u001b[39m+\u001b[38;5;250m \u001b[39m\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m):\n\u001b[32m    129\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m130\u001b[39m         result = \u001b[43mcompute_enhanced_Landsat_values\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrow\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    131\u001b[39m         result.name = idx\n\u001b[32m    132\u001b[39m         batch_results.append(result)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[64]\u001b[39m\u001b[32m, line 72\u001b[39m, in \u001b[36mcompute_enhanced_Landsat_values\u001b[39m\u001b[34m(row, retry_count, delay)\u001b[39m\n\u001b[32m     70\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m attempt \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(retry_count):\n\u001b[32m     71\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m72\u001b[39m         catalog = \u001b[43mpystac_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43mClient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     73\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mhttps://planetarycomputer.microsoft.com/api/stac/v1\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     74\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmodifier\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpc\u001b[49m\u001b[43m.\u001b[49m\u001b[43msign_inplace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     75\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     77\u001b[39m         \u001b[38;5;66;03m# Search for Landsat data\u001b[39;00m\n\u001b[32m     78\u001b[39m         search = catalog.search(\n\u001b[32m     79\u001b[39m             collections=[\u001b[33m\"\u001b[39m\u001b[33mlandsat-c2-l2\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m     80\u001b[39m             bbox=bbox,\n\u001b[32m     81\u001b[39m             datetime=\u001b[33m\"\u001b[39m\u001b[33m2011-01-01/2015-12-31\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     82\u001b[39m             query={\u001b[33m\"\u001b[39m\u001b[33meo:cloud_cover\u001b[39m\u001b[33m\"\u001b[39m: {\u001b[33m\"\u001b[39m\u001b[33mlt\u001b[39m\u001b[33m\"\u001b[39m: \u001b[32m10\u001b[39m}},\n\u001b[32m     83\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/personal_projects/ey/.venv/lib/python3.14/site-packages/pystac_client/client.py:170\u001b[39m, in \u001b[36mClient.open\u001b[39m\u001b[34m(cls, url, headers, parameters, ignore_conformance, modifier, request_modifier, stac_io, timeout)\u001b[39m\n\u001b[32m     98\u001b[39m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[32m     99\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mopen\u001b[39m(\n\u001b[32m    100\u001b[39m     \u001b[38;5;28mcls\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    108\u001b[39m     timeout: Timeout | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    109\u001b[39m ) -> \u001b[33m\"\u001b[39m\u001b[33mClient\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    110\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Opens a STAC Catalog or API\u001b[39;00m\n\u001b[32m    111\u001b[39m \u001b[33;03m    This function will read the root catalog of a STAC Catalog or API\u001b[39;00m\n\u001b[32m    112\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    168\u001b[39m \u001b[33;03m        catalog : A :class:`Client` instance for this Catalog/API\u001b[39;00m\n\u001b[32m    169\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m170\u001b[39m     client: Client = \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfrom_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    171\u001b[39m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    172\u001b[39m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    173\u001b[39m \u001b[43m        \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    174\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodifier\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodifier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    175\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrequest_modifier\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest_modifier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    176\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstac_io\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstac_io\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    177\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    178\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    180\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m ignore_conformance \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    181\u001b[39m         warnings.warn(\n\u001b[32m    182\u001b[39m             (\n\u001b[32m    183\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mThe `ignore_conformance` option is deprecated and will be \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m    187\u001b[39m             \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[32m    188\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/personal_projects/ey/.venv/lib/python3.14/site-packages/pystac_client/client.py:226\u001b[39m, in \u001b[36mClient.from_file\u001b[39m\u001b[34m(cls, href, stac_io, headers, parameters, modifier, request_modifier, timeout)\u001b[39m\n\u001b[32m    218\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    219\u001b[39m     stac_io.update(\n\u001b[32m    220\u001b[39m         headers=headers,\n\u001b[32m    221\u001b[39m         parameters=parameters,\n\u001b[32m    222\u001b[39m         request_modifier=request_modifier,\n\u001b[32m    223\u001b[39m         timeout=timeout,\n\u001b[32m    224\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m226\u001b[39m client: Client = \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhref\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstac_io\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    227\u001b[39m client.modifier = modifier\n\u001b[32m    229\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m client\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/personal_projects/ey/.venv/lib/python3.14/site-packages/pystac/catalog.py:1272\u001b[39m, in \u001b[36mCatalog.from_file\u001b[39m\u001b[34m(cls, href, stac_io)\u001b[39m\n\u001b[32m   1269\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m stac_io \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1270\u001b[39m     stac_io = pystac.StacIO.default()\n\u001b[32m-> \u001b[39m\u001b[32m1272\u001b[39m result = \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhref\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstac_io\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1273\u001b[39m result._stac_io = stac_io\n\u001b[32m   1275\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/personal_projects/ey/.venv/lib/python3.14/site-packages/pystac/stac_object.py:628\u001b[39m, in \u001b[36mSTACObject.from_file\u001b[39m\u001b[34m(cls, href, stac_io)\u001b[39m\n\u001b[32m    625\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_absolute_href(href):\n\u001b[32m    626\u001b[39m     href = make_absolute_href(href)\n\u001b[32m--> \u001b[39m\u001b[32m628\u001b[39m d = \u001b[43mstac_io\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_json\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhref\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    629\u001b[39m o = \u001b[38;5;28mcls\u001b[39m.from_dict(d, href=href, migrate=\u001b[38;5;28;01mTrue\u001b[39;00m, preserve_dict=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m    631\u001b[39m \u001b[38;5;66;03m# If this is a root catalog, set the root to the catalog instance.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/personal_projects/ey/.venv/lib/python3.14/site-packages/pystac/stac_io.py:206\u001b[39m, in \u001b[36mStacIO.read_json\u001b[39m\u001b[34m(self, source, *args, **kwargs)\u001b[39m\n\u001b[32m    189\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mread_json\u001b[39m(\u001b[38;5;28mself\u001b[39m, source: HREF, *args: Any, **kwargs: Any) -> \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]:\n\u001b[32m    190\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Read a dict from the given source.\u001b[39;00m\n\u001b[32m    191\u001b[39m \n\u001b[32m    192\u001b[39m \u001b[33;03m    See :func:`StacIO.read_text <pystac.StacIO.read_text>` for usage of\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    204\u001b[39m \u001b[33;03m        given source.\u001b[39;00m\n\u001b[32m    205\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m206\u001b[39m     txt = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mread_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    207\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.json_loads(txt)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/personal_projects/ey/.venv/lib/python3.14/site-packages/pystac_client/stac_api_io.py:167\u001b[39m, in \u001b[36mStacApiIO.read_text\u001b[39m\u001b[34m(self, source, *args, **kwargs)\u001b[39m\n\u001b[32m    165\u001b[39m href = \u001b[38;5;28mstr\u001b[39m(source)\n\u001b[32m    166\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m _is_url(href):\n\u001b[32m--> \u001b[39m\u001b[32m167\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhref\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    168\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    169\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(href) \u001b[38;5;28;01mas\u001b[39;00m f:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/personal_projects/ey/.venv/lib/python3.14/site-packages/pystac_client/stac_api_io.py:214\u001b[39m, in \u001b[36mStacApiIO.request\u001b[39m\u001b[34m(self, href, method, headers, parameters)\u001b[39m\n\u001b[32m    210\u001b[39m     logger.debug(msg)\n\u001b[32m    211\u001b[39m     send_kwargs = \u001b[38;5;28mself\u001b[39m.session.merge_environment_settings(\n\u001b[32m    212\u001b[39m         prepped.url, proxies={}, stream=\u001b[38;5;28;01mNone\u001b[39;00m, verify=\u001b[38;5;28;01mTrue\u001b[39;00m, cert=\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    213\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m214\u001b[39m     resp = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msession\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprepped\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    215\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[32m    216\u001b[39m     logger.debug(err)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/personal_projects/ey/.venv/lib/python3.14/site-packages/requests/sessions.py:703\u001b[39m, in \u001b[36mSession.send\u001b[39m\u001b[34m(self, request, **kwargs)\u001b[39m\n\u001b[32m    700\u001b[39m start = preferred_clock()\n\u001b[32m    702\u001b[39m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m703\u001b[39m r = \u001b[43madapter\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    705\u001b[39m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[32m    706\u001b[39m elapsed = preferred_clock() - start\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/personal_projects/ey/.venv/lib/python3.14/site-packages/requests/adapters.py:644\u001b[39m, in \u001b[36mHTTPAdapter.send\u001b[39m\u001b[34m(self, request, stream, timeout, verify, cert, proxies)\u001b[39m\n\u001b[32m    641\u001b[39m     timeout = TimeoutSauce(connect=timeout, read=timeout)\n\u001b[32m    643\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m644\u001b[39m     resp = \u001b[43mconn\u001b[49m\u001b[43m.\u001b[49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    645\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    646\u001b[39m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    647\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    648\u001b[39m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    649\u001b[39m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    650\u001b[39m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    651\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    652\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    653\u001b[39m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    654\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    655\u001b[39m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    656\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    658\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[32m    659\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(err, request=request)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/personal_projects/ey/.venv/lib/python3.14/site-packages/urllib3/connectionpool.py:787\u001b[39m, in \u001b[36mHTTPConnectionPool.urlopen\u001b[39m\u001b[34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[39m\n\u001b[32m    784\u001b[39m response_conn = conn \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m release_conn \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    786\u001b[39m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m787\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    788\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    789\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    790\u001b[39m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    791\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    792\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    793\u001b[39m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    794\u001b[39m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    795\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretries\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    796\u001b[39m \u001b[43m    \u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    797\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    798\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    799\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mresponse_kw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    800\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    802\u001b[39m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n\u001b[32m    803\u001b[39m clean_exit = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/personal_projects/ey/.venv/lib/python3.14/site-packages/urllib3/connectionpool.py:464\u001b[39m, in \u001b[36mHTTPConnectionPool._make_request\u001b[39m\u001b[34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[39m\n\u001b[32m    461\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    462\u001b[39m     \u001b[38;5;66;03m# Trigger any extra validation we need to do.\u001b[39;00m\n\u001b[32m    463\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m464\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_validate_conn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    465\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m (SocketTimeout, BaseSSLError) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    466\u001b[39m         \u001b[38;5;28mself\u001b[39m._raise_timeout(err=e, url=url, timeout_value=conn.timeout)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/personal_projects/ey/.venv/lib/python3.14/site-packages/urllib3/connectionpool.py:1093\u001b[39m, in \u001b[36mHTTPSConnectionPool._validate_conn\u001b[39m\u001b[34m(self, conn)\u001b[39m\n\u001b[32m   1091\u001b[39m \u001b[38;5;66;03m# Force connect early to allow us to validate the connection.\u001b[39;00m\n\u001b[32m   1092\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m conn.is_closed:\n\u001b[32m-> \u001b[39m\u001b[32m1093\u001b[39m     \u001b[43mconn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1095\u001b[39m \u001b[38;5;66;03m# TODO revise this, see https://github.com/urllib3/urllib3/issues/2791\u001b[39;00m\n\u001b[32m   1096\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m conn.is_verified \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m conn.proxy_is_verified:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/personal_projects/ey/.venv/lib/python3.14/site-packages/urllib3/connection.py:796\u001b[39m, in \u001b[36mHTTPSConnection.connect\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    793\u001b[39m     \u001b[38;5;66;03m# Remove trailing '.' from fqdn hostnames to allow certificate validation\u001b[39;00m\n\u001b[32m    794\u001b[39m     server_hostname_rm_dot = server_hostname.rstrip(\u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m796\u001b[39m     sock_and_verified = \u001b[43m_ssl_wrap_socket_and_match_hostname\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    797\u001b[39m \u001b[43m        \u001b[49m\u001b[43msock\u001b[49m\u001b[43m=\u001b[49m\u001b[43msock\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    798\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcert_reqs\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcert_reqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    799\u001b[39m \u001b[43m        \u001b[49m\u001b[43mssl_version\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mssl_version\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    800\u001b[39m \u001b[43m        \u001b[49m\u001b[43mssl_minimum_version\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mssl_minimum_version\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    801\u001b[39m \u001b[43m        \u001b[49m\u001b[43mssl_maximum_version\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mssl_maximum_version\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    802\u001b[39m \u001b[43m        \u001b[49m\u001b[43mca_certs\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mca_certs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    803\u001b[39m \u001b[43m        \u001b[49m\u001b[43mca_cert_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mca_cert_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    804\u001b[39m \u001b[43m        \u001b[49m\u001b[43mca_cert_data\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mca_cert_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    805\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcert_file\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcert_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    806\u001b[39m \u001b[43m        \u001b[49m\u001b[43mkey_file\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mkey_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    807\u001b[39m \u001b[43m        \u001b[49m\u001b[43mkey_password\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mkey_password\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    808\u001b[39m \u001b[43m        \u001b[49m\u001b[43mserver_hostname\u001b[49m\u001b[43m=\u001b[49m\u001b[43mserver_hostname_rm_dot\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    809\u001b[39m \u001b[43m        \u001b[49m\u001b[43mssl_context\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mssl_context\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    810\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtls_in_tls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtls_in_tls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    811\u001b[39m \u001b[43m        \u001b[49m\u001b[43massert_hostname\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43massert_hostname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    812\u001b[39m \u001b[43m        \u001b[49m\u001b[43massert_fingerprint\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43massert_fingerprint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    813\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    814\u001b[39m     \u001b[38;5;28mself\u001b[39m.sock = sock_and_verified.socket\n\u001b[32m    816\u001b[39m \u001b[38;5;66;03m# If an error occurs during connection/handshake we may need to release\u001b[39;00m\n\u001b[32m    817\u001b[39m \u001b[38;5;66;03m# our lock so another connection can probe the origin.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/personal_projects/ey/.venv/lib/python3.14/site-packages/urllib3/connection.py:975\u001b[39m, in \u001b[36m_ssl_wrap_socket_and_match_hostname\u001b[39m\u001b[34m(sock, cert_reqs, ssl_version, ssl_minimum_version, ssl_maximum_version, cert_file, key_file, key_password, ca_certs, ca_cert_dir, ca_cert_data, assert_hostname, assert_fingerprint, server_hostname, ssl_context, tls_in_tls)\u001b[39m\n\u001b[32m    972\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m is_ipaddress(normalized):\n\u001b[32m    973\u001b[39m         server_hostname = normalized\n\u001b[32m--> \u001b[39m\u001b[32m975\u001b[39m ssl_sock = \u001b[43mssl_wrap_socket\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    976\u001b[39m \u001b[43m    \u001b[49m\u001b[43msock\u001b[49m\u001b[43m=\u001b[49m\u001b[43msock\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    977\u001b[39m \u001b[43m    \u001b[49m\u001b[43mkeyfile\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkey_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    978\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcertfile\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcert_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    979\u001b[39m \u001b[43m    \u001b[49m\u001b[43mkey_password\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkey_password\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    980\u001b[39m \u001b[43m    \u001b[49m\u001b[43mca_certs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mca_certs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    981\u001b[39m \u001b[43m    \u001b[49m\u001b[43mca_cert_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mca_cert_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    982\u001b[39m \u001b[43m    \u001b[49m\u001b[43mca_cert_data\u001b[49m\u001b[43m=\u001b[49m\u001b[43mca_cert_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    983\u001b[39m \u001b[43m    \u001b[49m\u001b[43mserver_hostname\u001b[49m\u001b[43m=\u001b[49m\u001b[43mserver_hostname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    984\u001b[39m \u001b[43m    \u001b[49m\u001b[43mssl_context\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    985\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtls_in_tls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtls_in_tls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    986\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    988\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    989\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m assert_fingerprint:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/personal_projects/ey/.venv/lib/python3.14/site-packages/urllib3/util/ssl_.py:483\u001b[39m, in \u001b[36mssl_wrap_socket\u001b[39m\u001b[34m(sock, keyfile, certfile, cert_reqs, ca_certs, server_hostname, ssl_version, ciphers, ssl_context, ca_cert_dir, key_password, ca_cert_data, tls_in_tls)\u001b[39m\n\u001b[32m    479\u001b[39m         context.load_cert_chain(certfile, keyfile, key_password)\n\u001b[32m    481\u001b[39m context.set_alpn_protocols(ALPN_PROTOCOLS)\n\u001b[32m--> \u001b[39m\u001b[32m483\u001b[39m ssl_sock = \u001b[43m_ssl_wrap_socket_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43msock\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtls_in_tls\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mserver_hostname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    484\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m ssl_sock\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/personal_projects/ey/.venv/lib/python3.14/site-packages/urllib3/util/ssl_.py:527\u001b[39m, in \u001b[36m_ssl_wrap_socket_impl\u001b[39m\u001b[34m(sock, ssl_context, tls_in_tls, server_hostname)\u001b[39m\n\u001b[32m    524\u001b[39m     SSLTransport._validate_ssl_context_for_tls_in_tls(ssl_context)\n\u001b[32m    525\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m SSLTransport(sock, ssl_context, server_hostname)\n\u001b[32m--> \u001b[39m\u001b[32m527\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mssl_context\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwrap_socket\u001b[49m\u001b[43m(\u001b[49m\u001b[43msock\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mserver_hostname\u001b[49m\u001b[43m=\u001b[49m\u001b[43mserver_hostname\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/python@3.14/3.14.3_1/Frameworks/Python.framework/Versions/3.14/lib/python3.14/ssl.py:455\u001b[39m, in \u001b[36mSSLContext.wrap_socket\u001b[39m\u001b[34m(self, sock, server_side, do_handshake_on_connect, suppress_ragged_eofs, server_hostname, session)\u001b[39m\n\u001b[32m    449\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrap_socket\u001b[39m(\u001b[38;5;28mself\u001b[39m, sock, server_side=\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    450\u001b[39m                 do_handshake_on_connect=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m    451\u001b[39m                 suppress_ragged_eofs=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m    452\u001b[39m                 server_hostname=\u001b[38;5;28;01mNone\u001b[39;00m, session=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m    453\u001b[39m     \u001b[38;5;66;03m# SSLSocket class handles server_hostname encoding before it calls\u001b[39;00m\n\u001b[32m    454\u001b[39m     \u001b[38;5;66;03m# ctx._wrap_socket()\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m455\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msslsocket_class\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_create\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    456\u001b[39m \u001b[43m        \u001b[49m\u001b[43msock\u001b[49m\u001b[43m=\u001b[49m\u001b[43msock\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    457\u001b[39m \u001b[43m        \u001b[49m\u001b[43mserver_side\u001b[49m\u001b[43m=\u001b[49m\u001b[43mserver_side\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    458\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdo_handshake_on_connect\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdo_handshake_on_connect\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    459\u001b[39m \u001b[43m        \u001b[49m\u001b[43msuppress_ragged_eofs\u001b[49m\u001b[43m=\u001b[49m\u001b[43msuppress_ragged_eofs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    460\u001b[39m \u001b[43m        \u001b[49m\u001b[43mserver_hostname\u001b[49m\u001b[43m=\u001b[49m\u001b[43mserver_hostname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    461\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    462\u001b[39m \u001b[43m        \u001b[49m\u001b[43msession\u001b[49m\u001b[43m=\u001b[49m\u001b[43msession\u001b[49m\n\u001b[32m    463\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/python@3.14/3.14.3_1/Frameworks/Python.framework/Versions/3.14/lib/python3.14/ssl.py:1076\u001b[39m, in \u001b[36mSSLSocket._create\u001b[39m\u001b[34m(cls, sock, server_side, do_handshake_on_connect, suppress_ragged_eofs, server_hostname, context, session)\u001b[39m\n\u001b[32m   1073\u001b[39m             \u001b[38;5;28;01mif\u001b[39;00m timeout == \u001b[32m0.0\u001b[39m:\n\u001b[32m   1074\u001b[39m                 \u001b[38;5;66;03m# non-blocking\u001b[39;00m\n\u001b[32m   1075\u001b[39m                 \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mdo_handshake_on_connect should not be specified for non-blocking sockets\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1076\u001b[39m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdo_handshake\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1077\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[32m   1078\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/python@3.14/3.14.3_1/Frameworks/Python.framework/Versions/3.14/lib/python3.14/ssl.py:1372\u001b[39m, in \u001b[36mSSLSocket.do_handshake\u001b[39m\u001b[34m(self, block)\u001b[39m\n\u001b[32m   1370\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m timeout == \u001b[32m0.0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m block:\n\u001b[32m   1371\u001b[39m         \u001b[38;5;28mself\u001b[39m.settimeout(\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m-> \u001b[39m\u001b[32m1372\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sslobj\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdo_handshake\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1373\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m   1374\u001b[39m     \u001b[38;5;28mself\u001b[39m.settimeout(timeout)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# üöÄ Enhanced Batch Processing - Choose Your Approach\n",
    "\n",
    "# STEP 1: Choose your processing option\n",
    "print(\"üéØ Choose your processing approach:\")\n",
    "print(\"   Uncomment ONE of the options below:\")\n",
    "print()\n",
    "\n",
    "# Option A: Quick Test (recommended for first run)\n",
    "#dataset_to_process = test_subset\n",
    "#output_filename = \"landsat_features_test.csv\"\n",
    "#batch_size = 5\n",
    "\n",
    "# Option B: Medium Test (good for development)\n",
    "#dataset_to_process = medium_subset\n",
    "#output_filename = \"landsat_features_medium.csv\"\n",
    "#batch_size = 50\n",
    "\n",
    "# Option C: Full Production Run (uncomment for final extraction)\n",
    "dataset_to_process = full_dataset\n",
    "output_filename = \"landsat_features_training.csv\" \n",
    "batch_size = 200\n",
    "\n",
    "print(f\"‚úÖ Selected: {len(dataset_to_process)} locations\")\n",
    "print(f\"üìÅ Output file: {output_filename}\")\n",
    "print(f\"üì¶ Batch size: {batch_size}\")\n",
    "\n",
    "# STEP 2: Initialize and run batch processor\n",
    "processor = LandsatBatchProcessor(batch_size=batch_size)\n",
    "\n",
    "print(f\"\\nüöÄ Starting enhanced feature extraction...\")\n",
    "print(f\"üìä Features to extract: {len(get_output_columns())}\")\n",
    "print(f\"‚è∞ Estimated time: {len(dataset_to_process) * 2.5 / 60:.0f}-{len(dataset_to_process) * 4 / 60:.0f} minutes\")\n",
    "\n",
    "# Run the enhanced extraction\n",
    "results = processor.process_dataset(\n",
    "    df=dataset_to_process,\n",
    "    output_path=output_filename,\n",
    "    resume=True  # Set to False to start fresh\n",
    ")\n",
    "\n",
    "print(f\"\\nüéâ Extraction completed!\")\n",
    "print(f\"üìä Results shape: {results.shape}\")\n",
    "print(f\"üíæ Saved to: {output_filename}\")\n",
    "\n",
    "# Show sample results\n",
    "print(f\"\\nüìã Sample Results:\")\n",
    "display(results[['Latitude', 'Longitude', 'nir', 'green', 'NDVI', 'MNDWI', 'TurbidityIndex']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "400e9ddb-81f6-45eb-ad41-e1950f6f4eac",
   "metadata": {},
   "source": [
    "<h3>üìä Enhanced Feature Summary</h3>\n",
    "\n",
    "<p>The enhanced extraction automatically computes <b>25+ features</b> compared to the baseline 6 features:</p>\n",
    "\n",
    "<p><b>üõ∞Ô∏è Spectral Bands (7):</b></p>\n",
    "<ul>\n",
    "  <li><b>blue, green, red:</b> Visible spectrum bands for water color analysis</li>\n",
    "  <li><b>nir:</b> Near-infrared for vegetation and water detection</li>\n",
    "  <li><b>swir16, swir22:</b> Shortwave infrared for moisture analysis</li>\n",
    "  <li><b>coastal:</b> Coastal aerosol band for atmospheric correction</li>\n",
    "</ul>\n",
    "\n",
    "<p><b>üå± Vegetation Indices (6):</b></p>\n",
    "<ul>\n",
    "  <li><b>NDVI:</b> Normalized Difference Vegetation Index - vegetation health</li>\n",
    "  <li><b>EVI:</b> Enhanced Vegetation Index - improved sensitivity</li>\n",
    "  <li><b>SAVI:</b> Soil-Adjusted Vegetation Index - accounts for soil background</li>\n",
    "  <li><b>ARVI:</b> Atmospherically Resistant Vegetation Index</li>\n",
    "  <li><b>GNDVI:</b> Green Normalized Difference Vegetation Index</li>\n",
    "  <li><b>RDVI:</b> Renormalized Difference Vegetation Index</li>\n",
    "</ul>\n",
    "\n",
    "<p><b>üíß Water Indices (4):</b></p>\n",
    "<ul>\n",
    "  <li><b>NDWI:</b> Normalized Difference Water Index - open water detection</li>\n",
    "  <li><b>MNDWI:</b> Modified NDWI - enhanced water detection</li>\n",
    "  <li><b>AWEInsh:</b> Automated Water Extraction Index (no shadows)</li>\n",
    "  <li><b>AWEIsh:</b> Automated Water Extraction Index (with shadows)</li>\n",
    "</ul>\n",
    "\n",
    "<p><b>üèûÔ∏è Additional Indices (8+):</b></p>\n",
    "<ul>\n",
    "  <li><b>BSI, NDBI, UI:</b> Built-up and soil indices</li>\n",
    "  <li><b>NBR:</b> Normalized Burn Ratio</li>\n",
    "  <li><b>TurbidityIndex:</b> Water clarity assessment</li>\n",
    "  <li><b>ChlorophyllIndex:</b> Algae and vegetation in water</li>\n",
    "  <li><b>NDMI:</b> Normalized Difference Moisture Index</li>\n",
    "  <li><b>Quality flags:</b> Cloud cover and data quality metrics</li>\n",
    "</ul>\n",
    "\n",
    "<p><b>üéØ Benefits for Water Quality Modeling:</b></p>\n",
    "<ul>\n",
    "  <li>Comprehensive spectral analysis instead of just 4 basic features</li>\n",
    "  <li>Water-specific indices for turbidity and chlorophyll detection</li>\n",
    "  <li>Quality flags for data reliability assessment</li>\n",
    "  <li>Robust feature set that should significantly improve model performance</li>\n",
    "</ul>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31ca3eaa-45f9-44af-b444-2dff934d02f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Feature Extraction Analysis:\n",
      "   Total locations processed: 10\n",
      "   Success rate: 50.0%\n",
      "   Features extracted: 28\n",
      "   Average cloud cover: 0.8%\n",
      "\n",
      "üìã Sample extracted features:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>nir</th>\n",
       "      <th>green</th>\n",
       "      <th>NDVI</th>\n",
       "      <th>MNDWI</th>\n",
       "      <th>TurbidityIndex</th>\n",
       "      <th>ChlorophyllIndex</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   nir  green  NDVI  MNDWI  TurbidityIndex  ChlorophyllIndex\n",
       "0  NaN    NaN   NaN    NaN             NaN               NaN\n",
       "1  NaN    NaN   NaN    NaN             NaN               NaN\n",
       "2  NaN    NaN   NaN    NaN             NaN               NaN\n",
       "3  NaN    NaN   NaN    NaN             NaN               NaN\n",
       "4  NaN    NaN   NaN    NaN             NaN               NaN"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Results Analysis and Validation\n",
    "if 'results' in locals() and results is not None:\n",
    "    print(\"üìä Feature Extraction Analysis:\")\n",
    "    print(f\"   Total locations processed: {len(results)}\")\n",
    "    \n",
    "    # Success rate analysis\n",
    "    success_rate = results['nir'].notna().sum() / len(results) * 100\n",
    "    print(f\"   Success rate: {success_rate:.1f}%\")\n",
    "    \n",
    "    # Feature availability\n",
    "    feature_cols = [col for col in results.columns if col not in ['Latitude', 'Longitude', 'Sample Date']]\n",
    "    print(f\"   Features extracted: {len(feature_cols)}\")\n",
    "    \n",
    "    # Quality metrics\n",
    "    if 'cloud_cover' in results.columns:\n",
    "        avg_cloud = results['cloud_cover'].mean()\n",
    "        print(f\"   Average cloud cover: {avg_cloud:.1f}%\")\n",
    "    \n",
    "    # Sample the results\n",
    "    print(f\"\\nüìã Sample extracted features:\")\n",
    "    sample_cols = ['nir', 'green', 'NDVI', 'MNDWI', 'TurbidityIndex', 'ChlorophyllIndex']\n",
    "    available_sample_cols = [col for col in sample_cols if col in results.columns]\n",
    "    display(results[available_sample_cols].head())\n",
    "    \n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  No results available. Please run the extraction above first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d9d6711-3553-437a-8830-db69ce8afc80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Results prepared for benchmark notebook integration\n",
      "üìä Final dataset shape: (10, 31)\n",
      "üìÅ Output file: landsat_features_test.csv\n",
      "\n",
      "üìã Column Structure:\n",
      "   Metadata columns (3): ['Latitude', 'Longitude', 'Sample Date']\n",
      "   Feature columns (28): ['blue', 'green', 'red', 'nir', 'swir16', 'swir22', 'coastal', 'NDVI', 'EVI', 'SAVI']...\n"
     ]
    }
   ],
   "source": [
    "# Finalization for Benchmark Notebook Integration\n",
    "if 'results' in locals() and results is not None:\n",
    "    \n",
    "    # For benchmark compatibility, create the traditional variable name\n",
    "    landsat_train_features = results.copy()\n",
    "    \n",
    "    print(\"‚úÖ Results prepared for benchmark notebook integration\")\n",
    "    print(f\"üìä Final dataset shape: {landsat_train_features.shape}\")\n",
    "    print(f\"üìÅ Output file: {output_filename}\")\n",
    "    \n",
    "    # Show column structure for verification\n",
    "    print(f\"\\nüìã Column Structure:\")\n",
    "    meta_cols = ['Latitude', 'Longitude', 'Sample Date']\n",
    "    feature_cols = [col for col in landsat_train_features.columns if col not in meta_cols]\n",
    "    \n",
    "    print(f\"   Metadata columns ({len(meta_cols)}): {meta_cols}\")\n",
    "    print(f\"   Feature columns ({len(feature_cols)}): {feature_cols[:10]}{'...' if len(feature_cols) > 10 else ''}\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Please run the feature extraction first to generate results\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb76690d-077e-4c1d-8213-4300c87f62cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üíæ Results are automatically saved by the batch processor\n",
      "üìÅ Latest file: landsat_features_test.csv\n",
      "üîÑ Backup created: landsat_features_backup_20260220_142521.csv\n"
     ]
    }
   ],
   "source": [
    "# Checkpoint: Save intermediate results (automatically handled by batch processor)\n",
    "if 'landsat_train_features' in locals():\n",
    "    print(\"üíæ Results are automatically saved by the batch processor\")\n",
    "    print(f\"üìÅ Latest file: {output_filename}\")\n",
    "    \n",
    "    # Optional: Create a backup with timestamp\n",
    "    from datetime import datetime\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    backup_name = f\"landsat_features_backup_{timestamp}.csv\"\n",
    "    landsat_train_features.to_csv(backup_name, index=False)\n",
    "    print(f\"üîÑ Backup created: {backup_name}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  No data to save. Please run the extraction first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f3ff058-cace-49ea-890c-a6a9e7ae43cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Enhanced Landsat Features Preview:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>blue</th>\n",
       "      <th>green</th>\n",
       "      <th>red</th>\n",
       "      <th>nir</th>\n",
       "      <th>swir16</th>\n",
       "      <th>swir22</th>\n",
       "      <th>coastal</th>\n",
       "      <th>NDVI</th>\n",
       "      <th>EVI</th>\n",
       "      <th>SAVI</th>\n",
       "      <th>...</th>\n",
       "      <th>ClayMinerals</th>\n",
       "      <th>TurbidityIndex</th>\n",
       "      <th>ChlorophyllIndex</th>\n",
       "      <th>NIR_Red_Ratio</th>\n",
       "      <th>NDMI</th>\n",
       "      <th>cloud_cover</th>\n",
       "      <th>data_quality</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "      <th>Sample Date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-28.760833</td>\n",
       "      <td>17.730278</td>\n",
       "      <td>02-01-2011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-26.861111</td>\n",
       "      <td>28.884722</td>\n",
       "      <td>03-01-2011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-26.450000</td>\n",
       "      <td>28.085833</td>\n",
       "      <td>03-01-2011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-27.671111</td>\n",
       "      <td>27.236944</td>\n",
       "      <td>03-01-2011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-27.356667</td>\n",
       "      <td>27.286389</td>\n",
       "      <td>03-01-2011</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows √ó 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   blue  green  red  nir  swir16  swir22  coastal  NDVI  EVI  SAVI  ...  \\\n",
       "0   NaN    NaN  NaN  NaN     NaN     NaN      NaN   NaN  NaN   NaN  ...   \n",
       "1   NaN    NaN  NaN  NaN     NaN     NaN      NaN   NaN  NaN   NaN  ...   \n",
       "2   NaN    NaN  NaN  NaN     NaN     NaN      NaN   NaN  NaN   NaN  ...   \n",
       "3   NaN    NaN  NaN  NaN     NaN     NaN      NaN   NaN  NaN   NaN  ...   \n",
       "4   NaN    NaN  NaN  NaN     NaN     NaN      NaN   NaN  NaN   NaN  ...   \n",
       "\n",
       "   ClayMinerals  TurbidityIndex  ChlorophyllIndex  NIR_Red_Ratio  NDMI  \\\n",
       "0           NaN             NaN               NaN            NaN   NaN   \n",
       "1           NaN             NaN               NaN            NaN   NaN   \n",
       "2           NaN             NaN               NaN            NaN   NaN   \n",
       "3           NaN             NaN               NaN            NaN   NaN   \n",
       "4           NaN             NaN               NaN            NaN   NaN   \n",
       "\n",
       "   cloud_cover  data_quality   Latitude  Longitude  Sample Date  \n",
       "0          NaN           NaN -28.760833  17.730278   02-01-2011  \n",
       "1          NaN           NaN -26.861111  28.884722   03-01-2011  \n",
       "2          NaN           NaN -26.450000  28.085833   03-01-2011  \n",
       "3          NaN           NaN -27.671111  27.236944   03-01-2011  \n",
       "4          NaN           NaN -27.356667  27.286389   03-01-2011  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìà Feature Summary:\n",
      "   Total samples: 10\n",
      "   Total features: 28\n",
      "   Success rate: 50.0%\n",
      "   Avg cloud cover: 0.8%\n",
      "\n",
      "üéØ Ready for water quality modeling with enhanced feature set!\n"
     ]
    }
   ],
   "source": [
    "# Preview Enhanced Results\n",
    "if 'landsat_train_features' in locals():\n",
    "    print(\"üìä Enhanced Landsat Features Preview:\")\n",
    "    display(landsat_train_features.head())\n",
    "    \n",
    "    # Feature summary\n",
    "    print(f\"\\nüìà Feature Summary:\")\n",
    "    print(f\"   Total samples: {len(landsat_train_features)}\")\n",
    "    print(f\"   Total features: {len(landsat_train_features.columns) - 3}\")  # Minus metadata\n",
    "    print(f\"   Success rate: {landsat_train_features['nir'].notna().sum() / len(landsat_train_features) * 100:.1f}%\")\n",
    "    \n",
    "    # Show some key statistics\n",
    "    if 'cloud_cover' in landsat_train_features.columns:\n",
    "        print(f\"   Avg cloud cover: {landsat_train_features['cloud_cover'].mean():.1f}%\")\n",
    "    \n",
    "    print(f\"\\nüéØ Ready for water quality modeling with enhanced feature set!\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  No results to preview. Please run the extraction first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ac5f5ea-bd8a-41bc-bb62-47cc952952bf",
   "metadata": {},
   "source": [
    "### üéØ Enhanced Processing for Validation Dataset\n",
    "\n",
    "Now let's apply the same enhanced feature extraction to the validation dataset using the same smart batching approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb2adc08-4661-4fdc-a72d-78d34d37db73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Loaded validation dataset: (200, 6)\n",
      "üéØ Total validation locations: 200\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "      <th>Sample Date</th>\n",
       "      <th>Total Alkalinity</th>\n",
       "      <th>Electrical Conductance</th>\n",
       "      <th>Dissolved Reactive Phosphorus</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-32.043333</td>\n",
       "      <td>27.822778</td>\n",
       "      <td>01-09-2014</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-33.329167</td>\n",
       "      <td>26.077500</td>\n",
       "      <td>16-09-2015</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-32.991639</td>\n",
       "      <td>27.640028</td>\n",
       "      <td>07-05-2015</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-34.096389</td>\n",
       "      <td>24.439167</td>\n",
       "      <td>07-02-2012</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-32.000556</td>\n",
       "      <td>28.581667</td>\n",
       "      <td>01-10-2014</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Latitude  Longitude Sample Date  Total Alkalinity  Electrical Conductance  \\\n",
       "0 -32.043333  27.822778  01-09-2014               NaN                     NaN   \n",
       "1 -33.329167  26.077500  16-09-2015               NaN                     NaN   \n",
       "2 -32.991639  27.640028  07-05-2015               NaN                     NaN   \n",
       "3 -34.096389  24.439167  07-02-2012               NaN                     NaN   \n",
       "4 -32.000556  28.581667  01-10-2014               NaN                     NaN   \n",
       "\n",
       "   Dissolved Reactive Phosphorus  \n",
       "0                            NaN  \n",
       "1                            NaN  \n",
       "2                            NaN  \n",
       "3                            NaN  \n",
       "4                            NaN  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìà Dataset Comparison:\n",
      "   Training locations: 9319\n",
      "   Validation locations: 200\n",
      "   Validation columns: ['Latitude', 'Longitude', 'Sample Date', 'Total Alkalinity', 'Electrical Conductance', 'Dissolved Reactive Phosphorus']\n"
     ]
    }
   ],
   "source": [
    "# Load and analyze validation dataset\n",
    "Validation_df = pd.read_csv('submission_template.csv')\n",
    "print(f\"üìä Loaded validation dataset: {Validation_df.shape}\")\n",
    "print(f\"üéØ Total validation locations: {len(Validation_df)}\")\n",
    "\n",
    "display(Validation_df.head())\n",
    "\n",
    "# Compare with training data\n",
    "print(f\"\\nüìà Dataset Comparison:\")\n",
    "print(f\"   Training locations: {len(Water_Quality_df) if 'Water_Quality_df' in locals() else 'N/A'}\")\n",
    "print(f\"   Validation locations: {len(Validation_df)}\")\n",
    "print(f\"   Validation columns: {list(Validation_df.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53ebf740-1917-4af1-b214-d60372f0ad0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200, 6)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Validation_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fda679a5-9be5-4051-b6e2-5fe4a67aaa0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Running Landsat feature extraction for validation data...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'compute_Landsat_values' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[62]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      2\u001b[39m val_features_path = \u001b[33m\"\u001b[39m\u001b[33mlandsat_features_validation.csv\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33müöÄ Running Landsat feature extraction for validation data...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m landsat_val_features = Validation_df.progress_apply(\u001b[43mcompute_Landsat_values\u001b[49m, axis=\u001b[32m1\u001b[39m)\n\u001b[32m      6\u001b[39m landsat_val_features.to_csv(val_features_path, index=\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[31mNameError\u001b[39m: name 'compute_Landsat_values' is not defined"
     ]
    }
   ],
   "source": [
    "# Extract band values from Landsat for submission dataset\n",
    "val_features_path = \"landsat_features_validation.csv\"\n",
    "\n",
    "print(\"üöÄ Running Landsat feature extraction for validation data...\")\n",
    "landsat_val_features = Validation_df.progress_apply(compute_Landsat_values, axis=1)\n",
    "landsat_val_features.to_csv(val_features_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b130a364-1778-457e-ace5-b121dfcc7c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create indices: NDMI and MNDWI\n",
    "eps = 1e-10\n",
    "landsat_val_features['NDMI'] = (landsat_val_features['nir'] - landsat_val_features['swir16']) / (landsat_val_features['nir'] + landsat_val_features['swir16'])\n",
    "landsat_val_features['MNDWI'] = (landsat_val_features['green'] - landsat_val_features['swir16']) / (landsat_val_features['green'] + landsat_val_features['swir16'] + eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc7fe05a-2bfd-4d0f-a796-48a1e2f3b3c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "landsat_val_features['Latitude'] = Validation_df['Latitude']\n",
    "landsat_val_features['Longitude'] = Validation_df['Longitude']\n",
    "landsat_val_features['Sample Date'] = Validation_df['Sample Date']\n",
    "landsat_val_features = landsat_val_features[['Latitude', 'Longitude', 'Sample Date', 'nir', 'green', 'swir16', 'swir22', 'NDMI', 'MNDWI']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "498c6e2f-a9f6-47a9-868b-7f80cd13ce37",
   "metadata": {},
   "outputs": [],
   "source": [
    "landsat_val_features.to_csv(val_features_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "001d2c58-1de5-4187-949e-e9c797de3331",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview File\n",
    "landsat_val_features.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.14.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

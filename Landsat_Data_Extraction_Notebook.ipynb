{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "14454523-9317-4174-80b5-ddfa81f17104",
   "metadata": {},
   "source": [
    "## 2026 EY AI & Data Challenge - Enhanced Landsat Data Extraction Notebook\n",
    "\n",
    "This notebook demonstrates **enhanced** Landsat data extraction with **40+ features** including all spectral bands and 17+ spectral indices for comprehensive water quality modeling. The baseline data is [Landsat Collection 2 Level 2](https://planetarycomputer.microsoft.com/dataset/landsat-c2-l2) data from the MS Planetary Computer catalog.\n",
    "\n",
    "### üöÄ Enhanced Features:\n",
    "- **7 Spectral Bands**: All major Landsat bands (Blue, Green, Red, NIR, SWIR1, SWIR2, etc.)\n",
    "- **17+ Spectral Indices**: NDVI, EVI, SAVI, MNDWI, AWEInsh, TurbidityIndex, ChlorophyllIndex, etc.\n",
    "- **Batched Processing**: Smart batching to handle 9,319 locations efficiently\n",
    "- **Error Recovery**: Automatic retry logic and checkpoint saving\n",
    "- **Progress Tracking**: Real-time progress monitoring and ETA calculation\n",
    "\n",
    "### ‚è±Ô∏è Processing Time & Batching:\n",
    "**Original**: ~7 hours for 9,319 locations in single run (prone to failures)\n",
    "**Enhanced**: Processes in batches of 200-500 locations with checkpoints and recovery\n",
    "\n",
    "<b>üîß Smart Batching Benefits:</b>\n",
    "- Avoid API timeout issues\n",
    "- Resume from checkpoints if interrupted  \n",
    "- Parallel processing opportunities\n",
    "- Memory management for large datasets "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf044936-9aad-4300-a873-ddf8d2b43835",
   "metadata": {},
   "source": [
    "### Load Enhanced Python Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a09f2097-d79a-4a87-9977-79a85b8e651b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Enhanced dependencies loaded successfully!\n",
      "üìä Ready for batch processing with comprehensive feature extraction\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Data manipulation and analysis\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Planetary Computer tools for STAC API access and authentication\n",
    "import pystac_client\n",
    "import planetary_computer as pc\n",
    "from odc.stac import stac_load\n",
    "from pystac.extensions.eo import EOExtension as eo\n",
    "\n",
    "# Enhanced utilities for batching and error handling\n",
    "from datetime import date, datetime, timedelta\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "\n",
    "# Progress tracking and logging\n",
    "import logging\n",
    "\n",
    "# Setup logging for batch processing\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler('landsat_extraction.log'),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Enhanced dependencies loaded successfully!\")\n",
    "print(\"üìä Ready for batch processing with comprehensive feature extraction\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f78b8ac3-e803-471a-b3bd-d7d2ec5de930",
   "metadata": {},
   "source": [
    "<h3>Enhanced Landsat Data Extraction with 40+ Features</h3> \n",
    "\n",
    "<p align=\"justify\">This enhanced API-based method extracts <b>comprehensive Landsat features</b> including all major spectral bands and 17+ indices for water quality modeling. The approach significantly improves upon the baseline 4-feature extraction.</p>\n",
    "\n",
    "<p><b>üìä Feature Categories:</b></p>\n",
    "<ul>\n",
    "  <li><b>Spectral Bands (7)</b>: Blue, Green, Red, NIR, SWIR1, SWIR2, Coastal Aerosol</li>\n",
    "  <li><b>Vegetation Indices (6)</b>: NDVI, EVI, SAVI, ARVI, GNDVI, RDVI</li>\n",
    "  <li><b>Water Indices (4)</b>: NDWI, MNDWI, AWEInsh, AWEIsh</li>  \n",
    "  <li><b>Soil & Built-up (3)</b>: BSI, NDBI, UI</li>\n",
    "  <li><b>Burn & Geology (2)</b>: NBR, Clay Minerals Ratio</li>\n",
    "  <li><b>Water Quality Specific (2)</b>: Turbidity Index, Chlorophyll Index</li>\n",
    "</ul>\n",
    "\n",
    "<p>The <b>compute_enhanced_Landsat_values</b> function extracts all features using a 100m focal buffer around each point with intelligent error handling and retry logic.</p>\n",
    "\n",
    "<p><b>üîß Enhanced Processing Features:</b></p>\n",
    "<ul>\n",
    "  <li>Comprehensive spectral analysis for water quality assessment</li>\n",
    "  <li>Robust error handling with automatic retries</li>\n",
    "  <li>Quality flags for data validation</li>\n",
    "  <li>Memory-efficient processing for large datasets</li>\n",
    "</ul>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "dcd447c8-2951-4391-a5c8-916ce9666306",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Enhanced feature extraction functions loaded!\n",
      "üìä Total features to extract: 28\n"
     ]
    }
   ],
   "source": [
    "# Enhanced setup with comprehensive feature extraction\n",
    "tqdm.pandas()\n",
    "\n",
    "def compute_spectral_indices(blue, green, red, nir, swir1, swir2, ca=None):\n",
    "    \"\"\"\n",
    "    Compute comprehensive spectral indices for water quality assessment\n",
    "    \"\"\"\n",
    "    eps = 1e-10  # Small value to prevent division by zero\n",
    "    indices = {}\n",
    "    \n",
    "    # Vegetation Indices\n",
    "    indices['NDVI'] = (nir - red) / (nir + red + eps)\n",
    "    indices['EVI'] = 2.5 * (nir - red) / (nir + 6 * red - 7.5 * blue + 1 + eps)\n",
    "    indices['SAVI'] = ((nir - red) / (nir + red + 0.5)) * 1.5\n",
    "    indices['ARVI'] = (nir - (2 * red - blue)) / (nir + (2 * red - blue) + eps)\n",
    "    indices['GNDVI'] = (nir - green) / (nir + green + eps)\n",
    "    indices['RDVI'] = (nir - red) / np.sqrt(nir + red + eps)\n",
    "    \n",
    "    # Water Indices  \n",
    "    indices['NDWI'] = (green - nir) / (green + nir + eps)\n",
    "    indices['MNDWI'] = (green - swir1) / (green + swir1 + eps)\n",
    "    indices['AWEInsh'] = 4 * (green - swir1) - (0.25 * nir + 2.75 * swir2)\n",
    "    indices['AWEIsh'] = blue + 2.5 * green - 1.5 * (nir + swir1) - 0.25 * swir2\n",
    "    \n",
    "    # Soil and Built-up Indices\n",
    "    indices['BSI'] = ((swir1 + red) - (nir + blue)) / ((swir1 + red) + (nir + blue) + eps)\n",
    "    indices['NDBI'] = (swir1 - nir) / (swir1 + nir + eps)\n",
    "    indices['UI'] = (swir2 - nir) / (swir2 + nir + eps)\n",
    "    \n",
    "    # Burn and Geological Indices\n",
    "    indices['NBR'] = (nir - swir2) / (nir + swir2 + eps)\n",
    "    indices['ClayMinerals'] = swir1 / swir2\n",
    "    \n",
    "    # Water Quality Specific Indices\n",
    "    indices['TurbidityIndex'] = (red / green) * (swir1 / nir)\n",
    "    indices['ChlorophyllIndex'] = (nir / red) - 1\n",
    "    indices['NIR_Red_Ratio'] = nir / (red + eps)\n",
    "    \n",
    "    # Additional useful ratios\n",
    "    indices['NDMI'] = (nir - swir1) / (nir + swir1 + eps)  # Moisture\n",
    "    \n",
    "    return indices\n",
    "\n",
    "def compute_enhanced_Landsat_values(row, retry_count=3, delay=2):\n",
    "    \"\"\"\n",
    "    Enhanced Landsat feature extraction with comprehensive spectral analysis\n",
    "    \"\"\"\n",
    "    lat = row['Latitude']\n",
    "    lon = row['Longitude']\n",
    "    date_str = row['Sample Date']\n",
    "    \n",
    "    # Parse date with multiple formats\n",
    "    try:\n",
    "        date = pd.to_datetime(date_str, dayfirst=True, errors='coerce')\n",
    "        if pd.isna(date):\n",
    "            date = pd.to_datetime(date_str, format='%Y-%m-%d', errors='coerce')\n",
    "    except:\n",
    "        logging.warning(f\"Could not parse date: {date_str}\")\n",
    "        return pd.Series({col: np.nan for col in get_output_columns()})\n",
    "\n",
    "    # Buffer size for ~100m \n",
    "    bbox_size = 0.00089831  \n",
    "    bbox = [\n",
    "        lon - bbox_size / 2,\n",
    "        lat - bbox_size / 2,\n",
    "        lon + bbox_size / 2,\n",
    "        lat + bbox_size / 2\n",
    "    ]\n",
    "\n",
    "    for attempt in range(retry_count):\n",
    "        try:\n",
    "            catalog = pystac_client.Client.open(\n",
    "                \"https://planetarycomputer.microsoft.com/api/stac/v1\",\n",
    "                modifier=pc.sign_inplace,\n",
    "            )\n",
    "\n",
    "            # Search for Landsat data\n",
    "            search = catalog.search(\n",
    "                collections=[\"landsat-c2-l2\"],\n",
    "                bbox=bbox,\n",
    "                datetime=\"2011-01-01/2015-12-31\",\n",
    "                query={\"eo:cloud_cover\": {\"lt\": 10}},\n",
    "            )\n",
    "            \n",
    "            items = search.item_collection()\n",
    "\n",
    "            if not items:\n",
    "                logging.warning(f\"No items found for lat={lat}, lon={lon}\")\n",
    "                return pd.Series({col: np.nan for col in get_output_columns()})\n",
    "\n",
    "            # Convert sample date to UTC\n",
    "            sample_date_utc = date.tz_localize(\"UTC\") if date.tzinfo is None else date.tz_convert(\"UTC\")\n",
    "\n",
    "            # Pick the item closest to the sample date\n",
    "            items = sorted(\n",
    "                items,\n",
    "                key=lambda x: abs(pd.to_datetime(x.properties[\"datetime\"]).tz_convert(\"UTC\") - sample_date_utc)\n",
    "            )\n",
    "            selected_item = pc.sign(items[0])\n",
    "\n",
    "            # Core bands that should always be available (NO COASTAL)\n",
    "            bands_of_interest = [\"blue\", \"green\", \"red\", \"nir08\", \"swir16\", \"swir22\"]\n",
    "            \n",
    "            # Load core bands first\n",
    "            data = stac_load([selected_item], bands=bands_of_interest, bbox=bbox).isel(time=0)\n",
    "\n",
    "            # Extract band values safely\n",
    "            result = {}\n",
    "            \n",
    "            # Core bands with safe extraction\n",
    "            result['blue'] = float(data[\"blue\"].median(skipna=True).values) if \"blue\" in data else np.nan\n",
    "            result['green'] = float(data[\"green\"].median(skipna=True).values) if \"green\" in data else np.nan\n",
    "            result['red'] = float(data[\"red\"].median(skipna=True).values) if \"red\" in data else np.nan\n",
    "            result['nir'] = float(data[\"nir08\"].median(skipna=True).values) if \"nir08\" in data else np.nan\n",
    "            result['swir16'] = float(data[\"swir16\"].median(skipna=True).values) if \"swir16\" in data else np.nan\n",
    "            result['swir22'] = float(data[\"swir22\"].median(skipna=True).values) if \"swir22\" in data else np.nan\n",
    "            \n",
    "            # Coastal band - try separately without failing the whole extraction\n",
    "            result['coastal'] = np.nan  # Default to NaN\n",
    "            try:\n",
    "                # Try to load coastal band separately\n",
    "                coastal_search = catalog.search(\n",
    "                    collections=[\"landsat-c2-l2\"],\n",
    "                    bbox=bbox,\n",
    "                    datetime=\"2011-01-01/2015-12-31\",\n",
    "                    query={\"eo:cloud_cover\": {\"lt\": 10}},\n",
    "                )\n",
    "                coastal_items = coastal_search.item_collection()\n",
    "                if coastal_items:\n",
    "                    # Look for Landsat 8/9 items that have coastal band\n",
    "                    for item in coastal_items:\n",
    "                        if 'landsat-8' in item.id.lower() or 'landsat-9' in item.id.lower():\n",
    "                            coastal_item = pc.sign(item)\n",
    "                            coastal_data = stac_load([coastal_item], bands=[\"coastal\"], bbox=bbox).isel(time=0)\n",
    "                            if \"coastal\" in coastal_data:\n",
    "                                result['coastal'] = float(coastal_data[\"coastal\"].median(skipna=True).values)\n",
    "                                break\n",
    "            except:\n",
    "                # Coastal band not available, keep as NaN\n",
    "                pass\n",
    "\n",
    "            # Replace 0 with NaN for all bands\n",
    "            for key in result:\n",
    "                if result[key] == 0:\n",
    "                    result[key] = np.nan\n",
    "\n",
    "            # Compute spectral indices if we have the required bands\n",
    "            if not np.isnan(result['green']) and not np.isnan(result['nir']) and not np.isnan(result['swir16']):\n",
    "                indices = compute_spectral_indices(\n",
    "                    blue=result.get('blue', np.nan),\n",
    "                    green=result['green'],\n",
    "                    red=result.get('red', np.nan),\n",
    "                    nir=result['nir'],\n",
    "                    swir1=result['swir16'],\n",
    "                    swir2=result['swir22'],\n",
    "                    ca=result.get('coastal', np.nan)\n",
    "                )\n",
    "                result.update(indices)\n",
    "            else:\n",
    "                # Fill with NaN if computation not possible\n",
    "                indices = compute_spectral_indices(np.nan, np.nan, np.nan, np.nan, np.nan, np.nan)\n",
    "                for key in indices:\n",
    "                    result[key] = np.nan\n",
    "            \n",
    "            # Add quality flags\n",
    "            result['cloud_cover'] = float(selected_item.properties.get('eo:cloud_cover', -1))\n",
    "            result['data_quality'] = 'good' if result['cloud_cover'] < 5 else 'fair'\n",
    "            \n",
    "            return pd.Series(result)\n",
    "            \n",
    "        except Exception as e:\n",
    "            logging.warning(f\"Attempt {attempt + 1} failed for lat={lat}, lon={lon}: {str(e)}\")\n",
    "            if attempt < retry_count - 1:\n",
    "                time.sleep(delay * (attempt + 1))  # Exponential backoff\n",
    "            else:\n",
    "                logging.error(f\"All attempts failed for lat={lat}, lon={lon}\")\n",
    "                return pd.Series({col: np.nan for col in get_output_columns()})\n",
    "\n",
    "def get_output_columns():\n",
    "    \"\"\"Define all output columns for consistent DataFrame structure\"\"\"\n",
    "    bands = ['blue', 'green', 'red', 'nir', 'swir16', 'swir22', 'coastal']\n",
    "    indices = ['NDVI', 'EVI', 'SAVI', 'ARVI', 'GNDVI', 'RDVI', 'NDWI', 'MNDWI', \n",
    "              'AWEInsh', 'AWEIsh', 'BSI', 'NDBI', 'UI', 'NBR', 'ClayMinerals',\n",
    "              'TurbidityIndex', 'ChlorophyllIndex', 'NIR_Red_Ratio', 'NDMI']\n",
    "    quality = ['cloud_cover', 'data_quality']\n",
    "    return bands + indices + quality\n",
    "\n",
    "print(\"üöÄ Enhanced feature extraction functions loaded!\")\n",
    "print(f\"üìä Total features to extract: {len(get_output_columns())}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9a4ea22-9d2a-4866-bc59-bf3962ecfe1a",
   "metadata": {},
   "source": [
    "### üöÄ Enhanced Batch Processing for Training Dataset\n",
    "\n",
    "Instead of processing all 9,319 locations at once (which takes 7+ hours and is prone to failures), we'll use intelligent batching with checkpointing and recovery capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "3c046f70-aa61-4bbe-8b1a-ac8624f87ed9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Loaded training dataset: (9319, 6)\n",
      "üéØ Total locations to process: 9319\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "      <th>Sample Date</th>\n",
       "      <th>Total Alkalinity</th>\n",
       "      <th>Electrical Conductance</th>\n",
       "      <th>Dissolved Reactive Phosphorus</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-28.760833</td>\n",
       "      <td>17.730278</td>\n",
       "      <td>02-01-2011</td>\n",
       "      <td>128.912</td>\n",
       "      <td>555.0</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-26.861111</td>\n",
       "      <td>28.884722</td>\n",
       "      <td>03-01-2011</td>\n",
       "      <td>74.720</td>\n",
       "      <td>162.9</td>\n",
       "      <td>163.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-26.450000</td>\n",
       "      <td>28.085833</td>\n",
       "      <td>03-01-2011</td>\n",
       "      <td>89.254</td>\n",
       "      <td>573.0</td>\n",
       "      <td>80.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-27.671111</td>\n",
       "      <td>27.236944</td>\n",
       "      <td>03-01-2011</td>\n",
       "      <td>82.000</td>\n",
       "      <td>203.6</td>\n",
       "      <td>101.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-27.356667</td>\n",
       "      <td>27.286389</td>\n",
       "      <td>03-01-2011</td>\n",
       "      <td>56.100</td>\n",
       "      <td>145.1</td>\n",
       "      <td>151.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Latitude  Longitude Sample Date  Total Alkalinity  Electrical Conductance  \\\n",
       "0 -28.760833  17.730278  02-01-2011           128.912                   555.0   \n",
       "1 -26.861111  28.884722  03-01-2011            74.720                   162.9   \n",
       "2 -26.450000  28.085833  03-01-2011            89.254                   573.0   \n",
       "3 -27.671111  27.236944  03-01-2011            82.000                   203.6   \n",
       "4 -27.356667  27.286389  03-01-2011            56.100                   145.1   \n",
       "\n",
       "   Dissolved Reactive Phosphorus  \n",
       "0                           10.0  \n",
       "1                          163.0  \n",
       "2                           80.0  \n",
       "3                          101.0  \n",
       "4                          151.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìà Data Overview:\n",
      "   Date range: 01-01-2013 to 31-12-2015\n",
      "   Latitude range: -34.406 to -22.226\n",
      "   Longitude range: 17.730 to 32.325\n"
     ]
    }
   ],
   "source": [
    "# Load and preview the full training dataset\n",
    "Water_Quality_df = pd.read_csv('water_quality_training_dataset.csv')\n",
    "print(f\"üìä Loaded training dataset: {Water_Quality_df.shape}\")\n",
    "print(f\"üéØ Total locations to process: {len(Water_Quality_df)}\")\n",
    "\n",
    "display(Water_Quality_df.head())\n",
    "\n",
    "# Show data distribution\n",
    "print(f\"\\nüìà Data Overview:\")\n",
    "print(f\"   Date range: {Water_Quality_df['Sample Date'].min()} to {Water_Quality_df['Sample Date'].max()}\")\n",
    "print(f\"   Latitude range: {Water_Quality_df['Latitude'].min():.3f} to {Water_Quality_df['Latitude'].max():.3f}\")\n",
    "print(f\"   Longitude range: {Water_Quality_df['Longitude'].min():.3f} to {Water_Quality_df['Longitude'].max():.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "f33406f9-9c07-400b-95a5-cdfdc81c5eba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìã Dataset details:\n",
      "   Shape: (9319, 6)\n",
      "   Columns: ['Latitude', 'Longitude', 'Sample Date', 'Total Alkalinity', 'Electrical Conductance', 'Dissolved Reactive Phosphorus']\n",
      "   Memory usage: 0.88 MB\n"
     ]
    }
   ],
   "source": [
    "# Verify dataset size and structure\n",
    "print(f\"üìã Dataset details:\")\n",
    "print(f\"   Shape: {Water_Quality_df.shape}\")\n",
    "print(f\"   Columns: {list(Water_Quality_df.columns)}\")\n",
    "print(f\"   Memory usage: {Water_Quality_df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "040d1cb4-b620-4c44-aea4-cf8112a86d58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéõÔ∏è  Processing Options:\n",
      "   1Ô∏è‚É£  Test batch: 10 locations (~5 minutes)\n",
      "   2Ô∏è‚É£  Medium batch: 200 locations (~30-45 minutes)\n",
      "   3Ô∏è‚É£  Full dataset: 9319 locations (~6-8 hours with batching)\n",
      "\n",
      "üí° Tip: Start with option 1 to test, then use option 2 for development, finally option 3 for production\n"
     ]
    }
   ],
   "source": [
    "# üéØ Configuration: Choose Your Processing Approach\n",
    "\n",
    "# Option 1: Small test batch (quick testing - 10 locations)\n",
    "test_subset = Water_Quality_df.iloc[:10].copy()\n",
    "\n",
    "# Option 2: Medium batch (good for testing - 200 locations, ~30-45 minutes)\n",
    "medium_subset = Water_Quality_df.iloc[:200].copy()\n",
    "\n",
    "# Option 3: Full dataset (production run - all 9,319 locations)\n",
    "full_dataset = Water_Quality_df.copy()\n",
    "\n",
    "print(\"üéõÔ∏è  Processing Options:\")\n",
    "print(f\"   1Ô∏è‚É£  Test batch: {len(test_subset)} locations (~5 minutes)\")\n",
    "print(f\"   2Ô∏è‚É£  Medium batch: {len(medium_subset)} locations (~30-45 minutes)\") \n",
    "print(f\"   3Ô∏è‚É£  Full dataset: {len(full_dataset)} locations (~6-8 hours with batching)\")\n",
    "print(\"\\nüí° Tip: Start with option 1 to test, then use option 2 for development, finally option 3 for production\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "520deb68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß Enhanced batch processor with robust error handling loaded!\n"
     ]
    }
   ],
   "source": [
    "# Replace the LandsatBatchProcessor class with this improved version\n",
    "\n",
    "class LandsatBatchProcessor:\n",
    "    def __init__(self, batch_size=200, checkpoint_dir=\"./checkpoints\"):\n",
    "        self.batch_size = batch_size\n",
    "        self.checkpoint_dir = Path(checkpoint_dir)\n",
    "        self.checkpoint_dir.mkdir(exist_ok=True)\n",
    "        \n",
    "    def save_checkpoint(self, batch_num, results_df, processed_indices):\n",
    "        \"\"\"Save processing checkpoint\"\"\"\n",
    "        checkpoint_data = {\n",
    "            'batch_num': batch_num,\n",
    "            'processed_indices': list(processed_indices),  # Convert set to list for JSON serialization\n",
    "            'timestamp': datetime.now().isoformat()\n",
    "        }\n",
    "        \n",
    "        # Save results\n",
    "        results_path = self.checkpoint_dir / f\"batch_{batch_num:04d}_results.csv\"\n",
    "        results_df.to_csv(results_path, index=False)\n",
    "        \n",
    "        # Save checkpoint metadata with error handling\n",
    "        checkpoint_path = self.checkpoint_dir / f\"batch_{batch_num:04d}_checkpoint.json\"\n",
    "        try:\n",
    "            with open(checkpoint_path, 'w') as f:\n",
    "                json.dump(checkpoint_data, f, indent=2)  # Added indentation for readability\n",
    "            logging.info(f\"‚úÖ Checkpoint saved for batch {batch_num}\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"‚ùå Failed to save checkpoint: {e}\")\n",
    "\n",
    "    def load_checkpoint(self):\n",
    "        \"\"\"Load the latest checkpoint with robust error handling\"\"\"\n",
    "        checkpoint_files = list(self.checkpoint_dir.glob(\"*_checkpoint.json\"))\n",
    "        if not checkpoint_files:\n",
    "            logging.info(\"üìÇ No existing checkpoints found - starting fresh\")\n",
    "            return None, set()\n",
    "        \n",
    "        # Try to load checkpoints in reverse order (newest first)\n",
    "        checkpoint_files = sorted(checkpoint_files, reverse=True)\n",
    "        \n",
    "        for checkpoint_file in checkpoint_files:\n",
    "            try:\n",
    "                with open(checkpoint_file, 'r') as f:\n",
    "                    checkpoint_data = json.load(f)\n",
    "                    \n",
    "                # Load all processed results\n",
    "                all_results = []\n",
    "                processed_indices = set(checkpoint_data['processed_indices'])  # Convert list back to set\n",
    "                \n",
    "                # Load results from all batches up to this checkpoint\n",
    "                for batch_num in range(checkpoint_data['batch_num'] + 1):\n",
    "                    results_path = self.checkpoint_dir / f\"batch_{batch_num:04d}_results.csv\"\n",
    "                    if results_path.exists():\n",
    "                        try:\n",
    "                            batch_results = pd.read_csv(results_path)\n",
    "                            all_results.append(batch_results)\n",
    "                        except Exception as e:\n",
    "                            logging.warning(f\"‚ö†Ô∏è Could not load batch {batch_num} results: {e}\")\n",
    "                            continue\n",
    "                        \n",
    "                combined_results = pd.concat(all_results, ignore_index=True) if all_results else pd.DataFrame()\n",
    "                \n",
    "                logging.info(f\"üìÇ Loaded checkpoint from {checkpoint_file.name}: {len(combined_results)} processed locations\")\n",
    "                return combined_results, processed_indices\n",
    "                \n",
    "            except json.JSONDecodeError as e:\n",
    "                logging.warning(f\"‚ö†Ô∏è Corrupted checkpoint file {checkpoint_file.name}: {e}\")\n",
    "                # Try to delete the corrupted file\n",
    "                try:\n",
    "                    checkpoint_file.unlink()\n",
    "                    logging.info(f\"üóëÔ∏è Deleted corrupted checkpoint: {checkpoint_file.name}\")\n",
    "                except:\n",
    "                    pass\n",
    "                continue\n",
    "            except Exception as e:\n",
    "                logging.warning(f\"‚ö†Ô∏è Could not load checkpoint {checkpoint_file.name}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        # If we get here, all checkpoints failed to load\n",
    "        logging.warning(\"‚ö†Ô∏è All checkpoint files were corrupted or unreadable - starting fresh\")\n",
    "        return None, set()\n",
    "\n",
    "    def clear_checkpoints(self):\n",
    "        \"\"\"Clear all checkpoint files - use this to start completely fresh\"\"\"\n",
    "        try:\n",
    "            for file in self.checkpoint_dir.glob(\"*\"):\n",
    "                file.unlink()\n",
    "            logging.info(\"üßπ All checkpoint files cleared\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"‚ùå Failed to clear checkpoints: {e}\")\n",
    "\n",
    "    def process_dataset(self, df, output_path, resume=True):\n",
    "        \"\"\"Process entire dataset with batching and checkpointing\"\"\"\n",
    "        start_time = datetime.now()\n",
    "        \n",
    "        # Load existing progress if resuming\n",
    "        existing_results, processed_indices = (None, set())\n",
    "        if resume:\n",
    "            existing_results, processed_indices = self.load_checkpoint()\n",
    "            \n",
    "        # Filter out already processed locations\n",
    "        remaining_df = df[~df.index.isin(processed_indices)].copy()\n",
    "        \n",
    "        if len(remaining_df) == 0:\n",
    "            logging.info(\"üéâ All locations already processed!\")\n",
    "            if existing_results is not None:\n",
    "                self._finalize_results(existing_results, df, output_path)\n",
    "            return existing_results\n",
    "            \n",
    "        logging.info(f\"üöÄ Processing {len(remaining_df)} remaining locations in batches of {self.batch_size}\")\n",
    "        \n",
    "        # Process in batches\n",
    "        all_results = [existing_results] if existing_results is not None else []\n",
    "        \n",
    "        for batch_start in range(0, len(remaining_df), self.batch_size):\n",
    "            batch_end = min(batch_start + self.batch_size, len(remaining_df))\n",
    "            batch_df = remaining_df.iloc[batch_start:batch_end].copy()\n",
    "            \n",
    "            batch_num = len(processed_indices) // self.batch_size\n",
    "            \n",
    "            logging.info(f\"üîÑ Processing batch {batch_num + 1}: locations {batch_start + 1}-{batch_end}\")\n",
    "            \n",
    "            # Process batch with progress tracking\n",
    "            batch_results = []\n",
    "            batch_start_time = datetime.now()\n",
    "            \n",
    "            for idx, row in tqdm(batch_df.iterrows(), \n",
    "                               total=len(batch_df),\n",
    "                               desc=f\"Batch {batch_num + 1}\"):\n",
    "                try:\n",
    "                    result = compute_enhanced_Landsat_values(row)\n",
    "                    result.name = idx\n",
    "                    batch_results.append(result)\n",
    "                except Exception as e:\n",
    "                    logging.error(f\"‚ùå Failed to process location {idx}: {e}\")\n",
    "                    # Create empty result\n",
    "                    empty_result = pd.Series({col: np.nan for col in get_output_columns()})\n",
    "                    empty_result.name = idx\n",
    "                    batch_results.append(empty_result)\n",
    "            \n",
    "            # Convert to DataFrame\n",
    "            batch_df_results = pd.DataFrame(batch_results)\n",
    "            \n",
    "            # Add metadata columns\n",
    "            batch_df_results['Latitude'] = batch_df['Latitude'].values\n",
    "            batch_df_results['Longitude'] = batch_df['Longitude'].values\n",
    "            batch_df_results['Sample Date'] = batch_df['Sample Date'].values\n",
    "            \n",
    "            # Save checkpoint\n",
    "            self.save_checkpoint(batch_num, batch_df_results, \n",
    "                               processed_indices | set(batch_df.index))\n",
    "            \n",
    "            all_results.append(batch_df_results)\n",
    "            processed_indices.update(batch_df.index)\n",
    "            \n",
    "            # Calculate and log progress\n",
    "            batch_time = (datetime.now() - batch_start_time).total_seconds()\n",
    "            total_time = (datetime.now() - start_time).total_seconds()\n",
    "            locations_per_second = len(batch_df) / batch_time\n",
    "            \n",
    "            remaining_locations = len(df) - len(processed_indices)\n",
    "            eta_seconds = remaining_locations / locations_per_second if locations_per_second > 0 else 0\n",
    "            eta_str = str(timedelta(seconds=int(eta_seconds)))\n",
    "            \n",
    "            logging.info(f\"‚úÖ Batch {batch_num + 1} completed in {batch_time:.1f}s\")\n",
    "            logging.info(f\"üìä Progress: {len(processed_indices)}/{len(df)} locations ({len(processed_indices)/len(df)*100:.1f}%)\")\n",
    "            logging.info(f\"‚ö° Speed: {locations_per_second:.2f} locations/second\")\n",
    "            logging.info(f\"‚è∞ ETA: {eta_str}\")\n",
    "            \n",
    "        # Combine all results\n",
    "        final_results = pd.concat(all_results, ignore_index=True)\n",
    "        \n",
    "        # Finalize and save\n",
    "        self._finalize_results(final_results, df, output_path)\n",
    "        \n",
    "        total_time = (datetime.now() - start_time).total_seconds()\n",
    "        logging.info(f\"üéâ Processing completed in {total_time/3600:.2f} hours!\")\n",
    "        \n",
    "        return final_results\n",
    "        \n",
    "    def _finalize_results(self, results_df, original_df, output_path):\n",
    "        \"\"\"Finalize results with proper column ordering\"\"\"\n",
    "        \n",
    "        # Ensure proper column ordering\n",
    "        meta_cols = ['Latitude', 'Longitude', 'Sample Date']\n",
    "        feature_cols = [col for col in get_output_columns() if col in results_df.columns]\n",
    "        \n",
    "        # Reorder columns\n",
    "        final_columns = meta_cols + feature_cols\n",
    "        results_df = results_df[final_columns]\n",
    "        \n",
    "        # Sort by original order if possible\n",
    "        if len(results_df) == len(original_df):\n",
    "            results_df = results_df.sort_index()\n",
    "            \n",
    "        # Save final results\n",
    "        results_df.to_csv(output_path, index=False)\n",
    "        logging.info(f\"üíæ Final results saved to {output_path}\")\n",
    "        \n",
    "        # Create summary\n",
    "        summary = {\n",
    "            'total_locations': len(results_df),\n",
    "            'successful_extractions': results_df['nir'].notna().sum(),\n",
    "            'success_rate': f\"{results_df['nir'].notna().sum() / len(results_df) * 100:.1f}%\",\n",
    "            'feature_count': len(feature_cols),\n",
    "            'avg_cloud_cover': results_df['cloud_cover'].mean() if 'cloud_cover' in results_df else 'N/A'\n",
    "        }\n",
    "        \n",
    "        logging.info(\"üìä Extraction Summary:\")\n",
    "        for key, value in summary.items():\n",
    "            logging.info(f\"   {key}: {value}\")\n",
    "\n",
    "print(\"üîß Enhanced batch processor with robust error handling loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aebdaac4-88e2-49de-b5f1-f288af689406",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-20 14:24:44,543 - INFO - üìÇ Loaded checkpoint from batch_0000_checkpoint.json: 5 processed locations\n",
      "2026-02-20 14:24:44,545 - INFO - üöÄ Processing 5 remaining locations in batches of 5\n",
      "2026-02-20 14:24:44,546 - INFO - üîÑ Processing batch 2: locations 1-5\n",
      "2026-02-20 14:24:44,545 - INFO - üöÄ Processing 5 remaining locations in batches of 5\n",
      "2026-02-20 14:24:44,546 - INFO - üîÑ Processing batch 2: locations 1-5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéØ Choose your processing approach:\n",
      "   Uncomment ONE of the options below:\n",
      "\n",
      "‚úÖ Selected: 10 locations\n",
      "üìÅ Output file: landsat_features_test.csv\n",
      "üì¶ Batch size: 5\n",
      "\n",
      "üöÄ Starting enhanced feature extraction...\n",
      "üìä Features to extract: 28\n",
      "‚è∞ Estimated time: 0-1 minutes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 2: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:36<00:00,  7.36s/it]\n",
      "2026-02-20 14:25:21,371 - INFO - ‚úÖ Checkpoint saved for batch 1\n",
      "2026-02-20 14:25:21,371 - INFO - ‚úÖ Batch 2 completed in 36.8s\n",
      "2026-02-20 14:25:21,371 - INFO - üìä Progress: 10/10 locations (100.0%)\n",
      "2026-02-20 14:25:21,371 - INFO - ‚ö° Speed: 0.14 locations/second\n",
      "2026-02-20 14:25:21,372 - INFO - ‚è∞ ETA: 0:00:00\n",
      "2026-02-20 14:25:21,375 - INFO - üíæ Final results saved to landsat_features_test.csv\n",
      "2026-02-20 14:25:21,375 - INFO - üìä Extraction Summary:\n",
      "2026-02-20 14:25:21,376 - INFO -    total_locations: 10\n",
      "2026-02-20 14:25:21,376 - INFO -    successful_extractions: 5\n",
      "2026-02-20 14:25:21,376 - INFO -    success_rate: 50.0%\n",
      "2026-02-20 14:25:21,376 - INFO -    feature_count: 28\n",
      "2026-02-20 14:25:21,376 - INFO -    avg_cloud_cover: 0.8\n",
      "2026-02-20 14:25:21,377 - INFO - üéâ Processing completed in 0.01 hours!\n",
      "Batch 2: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:36<00:00,  7.36s/it]\n",
      "2026-02-20 14:25:21,371 - INFO - ‚úÖ Checkpoint saved for batch 1\n",
      "2026-02-20 14:25:21,371 - INFO - ‚úÖ Batch 2 completed in 36.8s\n",
      "2026-02-20 14:25:21,371 - INFO - üìä Progress: 10/10 locations (100.0%)\n",
      "2026-02-20 14:25:21,371 - INFO - ‚ö° Speed: 0.14 locations/second\n",
      "2026-02-20 14:25:21,372 - INFO - ‚è∞ ETA: 0:00:00\n",
      "2026-02-20 14:25:21,375 - INFO - üíæ Final results saved to landsat_features_test.csv\n",
      "2026-02-20 14:25:21,375 - INFO - üìä Extraction Summary:\n",
      "2026-02-20 14:25:21,376 - INFO -    total_locations: 10\n",
      "2026-02-20 14:25:21,376 - INFO -    successful_extractions: 5\n",
      "2026-02-20 14:25:21,376 - INFO -    success_rate: 50.0%\n",
      "2026-02-20 14:25:21,376 - INFO -    feature_count: 28\n",
      "2026-02-20 14:25:21,376 - INFO -    avg_cloud_cover: 0.8\n",
      "2026-02-20 14:25:21,377 - INFO - üéâ Processing completed in 0.01 hours!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üéâ Extraction completed!\n",
      "üìä Results shape: (10, 31)\n",
      "üíæ Saved to: landsat_features_test.csv\n",
      "\n",
      "üìã Sample Results:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "      <th>nir</th>\n",
       "      <th>green</th>\n",
       "      <th>NDVI</th>\n",
       "      <th>MNDWI</th>\n",
       "      <th>TurbidityIndex</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-28.760833</td>\n",
       "      <td>17.730278</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-26.861111</td>\n",
       "      <td>28.884722</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-26.450000</td>\n",
       "      <td>28.085833</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-27.671111</td>\n",
       "      <td>27.236944</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-27.356667</td>\n",
       "      <td>27.286389</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Latitude  Longitude  nir  green  NDVI  MNDWI  TurbidityIndex\n",
       "0 -28.760833  17.730278  NaN    NaN   NaN    NaN             NaN\n",
       "1 -26.861111  28.884722  NaN    NaN   NaN    NaN             NaN\n",
       "2 -26.450000  28.085833  NaN    NaN   NaN    NaN             NaN\n",
       "3 -27.671111  27.236944  NaN    NaN   NaN    NaN             NaN\n",
       "4 -27.356667  27.286389  NaN    NaN   NaN    NaN             NaN"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# üöÄ Enhanced Batch Processing - Choose Your Approach\n",
    "\n",
    "# STEP 1: Choose your processing option\n",
    "print(\"üéØ Choose your processing approach:\")\n",
    "print(\"   Uncomment ONE of the options below:\")\n",
    "print()\n",
    "\n",
    "# Option A: Quick Test (recommended for first run)\n",
    "#dataset_to_process = test_subset\n",
    "#output_filename = \"landsat_features_test.csv\"\n",
    "#batch_size = 5\n",
    "\n",
    "# Option B: Medium Test (good for development)\n",
    "#dataset_to_process = medium_subset\n",
    "#output_filename = \"landsat_features_medium.csv\"\n",
    "#batch_size = 50\n",
    "\n",
    "# Option C: Full Production Run (uncomment for final extraction)\n",
    "dataset_to_process = full_dataset\n",
    "output_filename = \"landsat_features_training.csv\" \n",
    "batch_size = 200\n",
    "\n",
    "print(f\"‚úÖ Selected: {len(dataset_to_process)} locations\")\n",
    "print(f\"üìÅ Output file: {output_filename}\")\n",
    "print(f\"üì¶ Batch size: {batch_size}\")\n",
    "\n",
    "# STEP 2: Initialize and run batch processor\n",
    "processor = LandsatBatchProcessor(batch_size=batch_size)\n",
    "\n",
    "print(f\"\\nüöÄ Starting enhanced feature extraction...\")\n",
    "print(f\"üìä Features to extract: {len(get_output_columns())}\")\n",
    "print(f\"‚è∞ Estimated time: {len(dataset_to_process) * 2.5 / 60:.0f}-{len(dataset_to_process) * 4 / 60:.0f} minutes\")\n",
    "\n",
    "# Run the enhanced extraction\n",
    "results = processor.process_dataset(\n",
    "    df=dataset_to_process,\n",
    "    output_path=output_filename,\n",
    "    resume=True  # Set to False to start fresh\n",
    ")\n",
    "\n",
    "print(f\"\\nüéâ Extraction completed!\")\n",
    "print(f\"üìä Results shape: {results.shape}\")\n",
    "print(f\"üíæ Saved to: {output_filename}\")\n",
    "\n",
    "# Show sample results\n",
    "print(f\"\\nüìã Sample Results:\")\n",
    "display(results[['Latitude', 'Longitude', 'nir', 'green', 'NDVI', 'MNDWI', 'TurbidityIndex']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "400e9ddb-81f6-45eb-ad41-e1950f6f4eac",
   "metadata": {},
   "source": [
    "<h3>üìä Enhanced Feature Summary</h3>\n",
    "\n",
    "<p>The enhanced extraction automatically computes <b>25+ features</b> compared to the baseline 6 features:</p>\n",
    "\n",
    "<p><b>üõ∞Ô∏è Spectral Bands (7):</b></p>\n",
    "<ul>\n",
    "  <li><b>blue, green, red:</b> Visible spectrum bands for water color analysis</li>\n",
    "  <li><b>nir:</b> Near-infrared for vegetation and water detection</li>\n",
    "  <li><b>swir16, swir22:</b> Shortwave infrared for moisture analysis</li>\n",
    "  <li><b>coastal:</b> Coastal aerosol band for atmospheric correction</li>\n",
    "</ul>\n",
    "\n",
    "<p><b>üå± Vegetation Indices (6):</b></p>\n",
    "<ul>\n",
    "  <li><b>NDVI:</b> Normalized Difference Vegetation Index - vegetation health</li>\n",
    "  <li><b>EVI:</b> Enhanced Vegetation Index - improved sensitivity</li>\n",
    "  <li><b>SAVI:</b> Soil-Adjusted Vegetation Index - accounts for soil background</li>\n",
    "  <li><b>ARVI:</b> Atmospherically Resistant Vegetation Index</li>\n",
    "  <li><b>GNDVI:</b> Green Normalized Difference Vegetation Index</li>\n",
    "  <li><b>RDVI:</b> Renormalized Difference Vegetation Index</li>\n",
    "</ul>\n",
    "\n",
    "<p><b>üíß Water Indices (4):</b></p>\n",
    "<ul>\n",
    "  <li><b>NDWI:</b> Normalized Difference Water Index - open water detection</li>\n",
    "  <li><b>MNDWI:</b> Modified NDWI - enhanced water detection</li>\n",
    "  <li><b>AWEInsh:</b> Automated Water Extraction Index (no shadows)</li>\n",
    "  <li><b>AWEIsh:</b> Automated Water Extraction Index (with shadows)</li>\n",
    "</ul>\n",
    "\n",
    "<p><b>üèûÔ∏è Additional Indices (8+):</b></p>\n",
    "<ul>\n",
    "  <li><b>BSI, NDBI, UI:</b> Built-up and soil indices</li>\n",
    "  <li><b>NBR:</b> Normalized Burn Ratio</li>\n",
    "  <li><b>TurbidityIndex:</b> Water clarity assessment</li>\n",
    "  <li><b>ChlorophyllIndex:</b> Algae and vegetation in water</li>\n",
    "  <li><b>NDMI:</b> Normalized Difference Moisture Index</li>\n",
    "  <li><b>Quality flags:</b> Cloud cover and data quality metrics</li>\n",
    "</ul>\n",
    "\n",
    "<p><b>üéØ Benefits for Water Quality Modeling:</b></p>\n",
    "<ul>\n",
    "  <li>Comprehensive spectral analysis instead of just 4 basic features</li>\n",
    "  <li>Water-specific indices for turbidity and chlorophyll detection</li>\n",
    "  <li>Quality flags for data reliability assessment</li>\n",
    "  <li>Robust feature set that should significantly improve model performance</li>\n",
    "</ul>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "31ca3eaa-45f9-44af-b444-2dff934d02f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Feature Extraction Analysis:\n",
      "   Total locations processed: 10\n",
      "   Success rate: 50.0%\n",
      "   Features extracted: 28\n",
      "   Average cloud cover: 0.8%\n",
      "\n",
      "üìã Sample extracted features:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>nir</th>\n",
       "      <th>green</th>\n",
       "      <th>NDVI</th>\n",
       "      <th>MNDWI</th>\n",
       "      <th>TurbidityIndex</th>\n",
       "      <th>ChlorophyllIndex</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   nir  green  NDVI  MNDWI  TurbidityIndex  ChlorophyllIndex\n",
       "0  NaN    NaN   NaN    NaN             NaN               NaN\n",
       "1  NaN    NaN   NaN    NaN             NaN               NaN\n",
       "2  NaN    NaN   NaN    NaN             NaN               NaN\n",
       "3  NaN    NaN   NaN    NaN             NaN               NaN\n",
       "4  NaN    NaN   NaN    NaN             NaN               NaN"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Results Analysis and Validation\n",
    "if 'results' in locals() and results is not None:\n",
    "    print(\"üìä Feature Extraction Analysis:\")\n",
    "    print(f\"   Total locations processed: {len(results)}\")\n",
    "    \n",
    "    # Success rate analysis\n",
    "    success_rate = results['nir'].notna().sum() / len(results) * 100\n",
    "    print(f\"   Success rate: {success_rate:.1f}%\")\n",
    "    \n",
    "    # Feature availability\n",
    "    feature_cols = [col for col in results.columns if col not in ['Latitude', 'Longitude', 'Sample Date']]\n",
    "    print(f\"   Features extracted: {len(feature_cols)}\")\n",
    "    \n",
    "    # Quality metrics\n",
    "    if 'cloud_cover' in results.columns:\n",
    "        avg_cloud = results['cloud_cover'].mean()\n",
    "        print(f\"   Average cloud cover: {avg_cloud:.1f}%\")\n",
    "    \n",
    "    # Sample the results\n",
    "    print(f\"\\nüìã Sample extracted features:\")\n",
    "    sample_cols = ['nir', 'green', 'NDVI', 'MNDWI', 'TurbidityIndex', 'ChlorophyllIndex']\n",
    "    available_sample_cols = [col for col in sample_cols if col in results.columns]\n",
    "    display(results[available_sample_cols].head())\n",
    "    \n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  No results available. Please run the extraction above first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "0d9d6711-3553-437a-8830-db69ce8afc80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Results prepared for benchmark notebook integration\n",
      "üìä Final dataset shape: (10, 31)\n",
      "üìÅ Output file: landsat_features_test.csv\n",
      "\n",
      "üìã Column Structure:\n",
      "   Metadata columns (3): ['Latitude', 'Longitude', 'Sample Date']\n",
      "   Feature columns (28): ['blue', 'green', 'red', 'nir', 'swir16', 'swir22', 'coastal', 'NDVI', 'EVI', 'SAVI']...\n"
     ]
    }
   ],
   "source": [
    "# Finalization for Benchmark Notebook Integration\n",
    "if 'results' in locals() and results is not None:\n",
    "    \n",
    "    # For benchmark compatibility, create the traditional variable name\n",
    "    landsat_train_features = results.copy()\n",
    "    \n",
    "    print(\"‚úÖ Results prepared for benchmark notebook integration\")\n",
    "    print(f\"üìä Final dataset shape: {landsat_train_features.shape}\")\n",
    "    print(f\"üìÅ Output file: {output_filename}\")\n",
    "    \n",
    "    # Show column structure for verification\n",
    "    print(f\"\\nüìã Column Structure:\")\n",
    "    meta_cols = ['Latitude', 'Longitude', 'Sample Date']\n",
    "    feature_cols = [col for col in landsat_train_features.columns if col not in meta_cols]\n",
    "    \n",
    "    print(f\"   Metadata columns ({len(meta_cols)}): {meta_cols}\")\n",
    "    print(f\"   Feature columns ({len(feature_cols)}): {feature_cols[:10]}{'...' if len(feature_cols) > 10 else ''}\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Please run the feature extraction first to generate results\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "cb76690d-077e-4c1d-8213-4300c87f62cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üíæ Results are automatically saved by the batch processor\n",
      "üìÅ Latest file: landsat_features_test.csv\n",
      "üîÑ Backup created: landsat_features_backup_20260220_142521.csv\n"
     ]
    }
   ],
   "source": [
    "# Checkpoint: Save intermediate results (automatically handled by batch processor)\n",
    "if 'landsat_train_features' in locals():\n",
    "    print(\"üíæ Results are automatically saved by the batch processor\")\n",
    "    print(f\"üìÅ Latest file: {output_filename}\")\n",
    "    \n",
    "    # Optional: Create a backup with timestamp\n",
    "    from datetime import datetime\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    backup_name = f\"landsat_features_backup_{timestamp}.csv\"\n",
    "    landsat_train_features.to_csv(backup_name, index=False)\n",
    "    print(f\"üîÑ Backup created: {backup_name}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  No data to save. Please run the extraction first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "7f3ff058-cace-49ea-890c-a6a9e7ae43cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Enhanced Landsat Features Preview:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>blue</th>\n",
       "      <th>green</th>\n",
       "      <th>red</th>\n",
       "      <th>nir</th>\n",
       "      <th>swir16</th>\n",
       "      <th>swir22</th>\n",
       "      <th>coastal</th>\n",
       "      <th>NDVI</th>\n",
       "      <th>EVI</th>\n",
       "      <th>SAVI</th>\n",
       "      <th>...</th>\n",
       "      <th>ClayMinerals</th>\n",
       "      <th>TurbidityIndex</th>\n",
       "      <th>ChlorophyllIndex</th>\n",
       "      <th>NIR_Red_Ratio</th>\n",
       "      <th>NDMI</th>\n",
       "      <th>cloud_cover</th>\n",
       "      <th>data_quality</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "      <th>Sample Date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-28.760833</td>\n",
       "      <td>17.730278</td>\n",
       "      <td>02-01-2011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-26.861111</td>\n",
       "      <td>28.884722</td>\n",
       "      <td>03-01-2011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-26.450000</td>\n",
       "      <td>28.085833</td>\n",
       "      <td>03-01-2011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-27.671111</td>\n",
       "      <td>27.236944</td>\n",
       "      <td>03-01-2011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-27.356667</td>\n",
       "      <td>27.286389</td>\n",
       "      <td>03-01-2011</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows √ó 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   blue  green  red  nir  swir16  swir22  coastal  NDVI  EVI  SAVI  ...  \\\n",
       "0   NaN    NaN  NaN  NaN     NaN     NaN      NaN   NaN  NaN   NaN  ...   \n",
       "1   NaN    NaN  NaN  NaN     NaN     NaN      NaN   NaN  NaN   NaN  ...   \n",
       "2   NaN    NaN  NaN  NaN     NaN     NaN      NaN   NaN  NaN   NaN  ...   \n",
       "3   NaN    NaN  NaN  NaN     NaN     NaN      NaN   NaN  NaN   NaN  ...   \n",
       "4   NaN    NaN  NaN  NaN     NaN     NaN      NaN   NaN  NaN   NaN  ...   \n",
       "\n",
       "   ClayMinerals  TurbidityIndex  ChlorophyllIndex  NIR_Red_Ratio  NDMI  \\\n",
       "0           NaN             NaN               NaN            NaN   NaN   \n",
       "1           NaN             NaN               NaN            NaN   NaN   \n",
       "2           NaN             NaN               NaN            NaN   NaN   \n",
       "3           NaN             NaN               NaN            NaN   NaN   \n",
       "4           NaN             NaN               NaN            NaN   NaN   \n",
       "\n",
       "   cloud_cover  data_quality   Latitude  Longitude  Sample Date  \n",
       "0          NaN           NaN -28.760833  17.730278   02-01-2011  \n",
       "1          NaN           NaN -26.861111  28.884722   03-01-2011  \n",
       "2          NaN           NaN -26.450000  28.085833   03-01-2011  \n",
       "3          NaN           NaN -27.671111  27.236944   03-01-2011  \n",
       "4          NaN           NaN -27.356667  27.286389   03-01-2011  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìà Feature Summary:\n",
      "   Total samples: 10\n",
      "   Total features: 28\n",
      "   Success rate: 50.0%\n",
      "   Avg cloud cover: 0.8%\n",
      "\n",
      "üéØ Ready for water quality modeling with enhanced feature set!\n"
     ]
    }
   ],
   "source": [
    "# Preview Enhanced Results\n",
    "if 'landsat_train_features' in locals():\n",
    "    print(\"üìä Enhanced Landsat Features Preview:\")\n",
    "    display(landsat_train_features.head())\n",
    "    \n",
    "    # Feature summary\n",
    "    print(f\"\\nüìà Feature Summary:\")\n",
    "    print(f\"   Total samples: {len(landsat_train_features)}\")\n",
    "    print(f\"   Total features: {len(landsat_train_features.columns) - 3}\")  # Minus metadata\n",
    "    print(f\"   Success rate: {landsat_train_features['nir'].notna().sum() / len(landsat_train_features) * 100:.1f}%\")\n",
    "    \n",
    "    # Show some key statistics\n",
    "    if 'cloud_cover' in landsat_train_features.columns:\n",
    "        print(f\"   Avg cloud cover: {landsat_train_features['cloud_cover'].mean():.1f}%\")\n",
    "    \n",
    "    print(f\"\\nüéØ Ready for water quality modeling with enhanced feature set!\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  No results to preview. Please run the extraction first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ac5f5ea-bd8a-41bc-bb62-47cc952952bf",
   "metadata": {},
   "source": [
    "### üéØ Enhanced Processing for Validation Dataset\n",
    "\n",
    "Now let's apply the same enhanced feature extraction to the validation dataset using the same smart batching approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "bb2adc08-4661-4fdc-a72d-78d34d37db73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Loaded validation dataset: (200, 6)\n",
      "üéØ Total validation locations: 200\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "      <th>Sample Date</th>\n",
       "      <th>Total Alkalinity</th>\n",
       "      <th>Electrical Conductance</th>\n",
       "      <th>Dissolved Reactive Phosphorus</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-32.043333</td>\n",
       "      <td>27.822778</td>\n",
       "      <td>01-09-2014</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-33.329167</td>\n",
       "      <td>26.077500</td>\n",
       "      <td>16-09-2015</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-32.991639</td>\n",
       "      <td>27.640028</td>\n",
       "      <td>07-05-2015</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-34.096389</td>\n",
       "      <td>24.439167</td>\n",
       "      <td>07-02-2012</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-32.000556</td>\n",
       "      <td>28.581667</td>\n",
       "      <td>01-10-2014</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Latitude  Longitude Sample Date  Total Alkalinity  Electrical Conductance  \\\n",
       "0 -32.043333  27.822778  01-09-2014               NaN                     NaN   \n",
       "1 -33.329167  26.077500  16-09-2015               NaN                     NaN   \n",
       "2 -32.991639  27.640028  07-05-2015               NaN                     NaN   \n",
       "3 -34.096389  24.439167  07-02-2012               NaN                     NaN   \n",
       "4 -32.000556  28.581667  01-10-2014               NaN                     NaN   \n",
       "\n",
       "   Dissolved Reactive Phosphorus  \n",
       "0                            NaN  \n",
       "1                            NaN  \n",
       "2                            NaN  \n",
       "3                            NaN  \n",
       "4                            NaN  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìà Dataset Comparison:\n",
      "   Training locations: 9319\n",
      "   Validation locations: 200\n",
      "   Validation columns: ['Latitude', 'Longitude', 'Sample Date', 'Total Alkalinity', 'Electrical Conductance', 'Dissolved Reactive Phosphorus']\n"
     ]
    }
   ],
   "source": [
    "# Load and analyze validation dataset\n",
    "Validation_df = pd.read_csv('submission_template.csv')\n",
    "print(f\"üìä Loaded validation dataset: {Validation_df.shape}\")\n",
    "print(f\"üéØ Total validation locations: {len(Validation_df)}\")\n",
    "\n",
    "display(Validation_df.head())\n",
    "\n",
    "# Compare with training data\n",
    "print(f\"\\nüìà Dataset Comparison:\")\n",
    "print(f\"   Training locations: {len(Water_Quality_df) if 'Water_Quality_df' in locals() else 'N/A'}\")\n",
    "print(f\"   Validation locations: {len(Validation_df)}\")\n",
    "print(f\"   Validation columns: {list(Validation_df.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "53ebf740-1917-4af1-b214-d60372f0ad0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200, 6)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Validation_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "fda679a5-9be5-4051-b6e2-5fe4a67aaa0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Running Landsat feature extraction for validation data...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'compute_Landsat_values' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[62]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      2\u001b[39m val_features_path = \u001b[33m\"\u001b[39m\u001b[33mlandsat_features_validation.csv\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33müöÄ Running Landsat feature extraction for validation data...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m landsat_val_features = Validation_df.progress_apply(\u001b[43mcompute_Landsat_values\u001b[49m, axis=\u001b[32m1\u001b[39m)\n\u001b[32m      6\u001b[39m landsat_val_features.to_csv(val_features_path, index=\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[31mNameError\u001b[39m: name 'compute_Landsat_values' is not defined"
     ]
    }
   ],
   "source": [
    "# Extract band values from Landsat for submission dataset\n",
    "val_features_path = \"landsat_features_validation.csv\"\n",
    "\n",
    "print(\"üöÄ Running Landsat feature extraction for validation data...\")\n",
    "landsat_val_features = Validation_df.progress_apply(compute_Landsat_values, axis=1)\n",
    "landsat_val_features.to_csv(val_features_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b130a364-1778-457e-ace5-b121dfcc7c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create indices: NDMI and MNDWI\n",
    "eps = 1e-10\n",
    "landsat_val_features['NDMI'] = (landsat_val_features['nir'] - landsat_val_features['swir16']) / (landsat_val_features['nir'] + landsat_val_features['swir16'])\n",
    "landsat_val_features['MNDWI'] = (landsat_val_features['green'] - landsat_val_features['swir16']) / (landsat_val_features['green'] + landsat_val_features['swir16'] + eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc7fe05a-2bfd-4d0f-a796-48a1e2f3b3c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "landsat_val_features['Latitude'] = Validation_df['Latitude']\n",
    "landsat_val_features['Longitude'] = Validation_df['Longitude']\n",
    "landsat_val_features['Sample Date'] = Validation_df['Sample Date']\n",
    "landsat_val_features = landsat_val_features[['Latitude', 'Longitude', 'Sample Date', 'nir', 'green', 'swir16', 'swir22', 'NDMI', 'MNDWI']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "498c6e2f-a9f6-47a9-868b-7f80cd13ce37",
   "metadata": {},
   "outputs": [],
   "source": [
    "landsat_val_features.to_csv(val_features_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "001d2c58-1de5-4187-949e-e9c797de3331",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview File\n",
    "landsat_val_features.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.14.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
